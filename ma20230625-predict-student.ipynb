{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22ae679",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-26T06:56:16.796777Z",
     "iopub.status.busy": "2023-06-26T06:56:16.796063Z",
     "iopub.status.idle": "2023-06-26T06:56:16.824859Z",
     "shell.execute_reply": "2023-06-26T06:56:16.824040Z"
    },
    "papermill": {
     "duration": 0.037167,
     "end_time": "2023-06-26T06:56:16.826995",
     "exception": false,
     "start_time": "2023-06-26T06:56:16.789828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import jo_wilder_310\n",
    "env = jo_wilder_310.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be2c44e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-26T06:56:16.837177Z",
     "iopub.status.busy": "2023-06-26T06:56:16.836570Z",
     "iopub.status.idle": "2023-06-26T06:56:22.637902Z",
     "shell.execute_reply": "2023-06-26T06:56:22.636673Z"
    },
    "papermill": {
     "duration": 5.809054,
     "end_time": "2023-06-26T06:56:22.640305",
     "exception": false,
     "start_time": "2023-06-26T06:56:16.831251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math \n",
    "import json\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "import polars as pl\n",
    "import multiprocessing \n",
    "from catboost import CatBoostClassifier,CatBoostRegressor,Pool\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "\n",
    "class Parameter(object):\n",
    "    def __init__(self):\n",
    "        # data\n",
    "        # This constructor initializes several variables that will be used as parameters in later computations.\n",
    "        # The values are set to their defaults upon the creation of a Parameter object.\n",
    "\n",
    "        self.user_dir =  './user_data/'  # The directory where user data will be stored.\n",
    "        self.k_folds = 5  # The number of folds to use in k-fold cross-validation.\n",
    "        self.n_jobs = 2  # The number of CPU cores to use for parallel execution.\n",
    "        self.random_seed = 27  # The seed value for the random number generator.\n",
    "        self.seq_length = 2560  # The length of the sequences for sequence-based computations.\n",
    "        self.cpu_cnt = multiprocessing.cpu_count()  # The number of CPUs available on the system.\n",
    "        self.use_cuda = torch.cuda.is_available()  # Boolean flag to determine if CUDA is available for use.\n",
    "        self.gpu = 0  # The GPU to use (0 indicates the first GPU).\n",
    "        self.print_freq = 1000  # The frequency at which messages are printed.\n",
    "        self.lr = 0.003  # The learning rate for the optimizer.\n",
    "        self.weight_decay = 0  # The weight decay (regularization term) to apply in the optimizer.\n",
    "        self.optim = 'Adam'  # The type of optimizer to use.\n",
    "        self.base_epoch = 30  # The base number of epochs for training.\n",
    "\n",
    "    def get(self, name):\n",
    "        # This function returns the value of the attribute specified by the name argument.\n",
    "        return getattr(self, name)\n",
    "\n",
    "    def set(self, **kwargs):\n",
    "        # This function sets the value of one or more attributes.\n",
    "        # The attribute names and their new values are provided as keyword arguments.\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "    def __str__(self):\n",
    "        # This function is a special method that returns a string representation of the Parameter object.\n",
    "        # It is called by built-in functions and operators that need to convert the object into a string.\n",
    "        # Here, it is returning a string that lists all the attribute names and their values.\n",
    "        return '\\n'.join(['%s:%s' % item for item in self.__dict__.items()])\n",
    "\n",
    "parameter = Parameter()\n",
    "    \n",
    "def processing_df(df, cat_dict):\n",
    "    df['level_group'] = df['level_group'].map({'0-4':0 ,'5-12':1, '13-22':2})\n",
    "    df = cat_encoder(df, cat_dict)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def get_features(df):\n",
    "    df = pl.from_pandas(df)\n",
    "    df = df.with_columns([(pl.col('elapsed_time') - pl.col('elapsed_time').shift(1)).over(['session_id']).alias('elapsed_time_diff'),\n",
    "                          (pl.col('room_coor_x') - pl.col('room_coor_x').shift(1))\n",
    "                             .over(['session_id']).abs().alias('room_coor_x_diff'),\n",
    "                          (pl.col('room_coor_y') - pl.col('room_coor_y').shift(1))\n",
    "                             .over(['session_id']).abs().alias('room_coor_y_diff'),\n",
    "                           ((pl.col('room_coor_x') - pl.col('room_coor_x').shift(1))\n",
    "                             .over(['session_id']) ** 2 + (pl.col('room_coor_y') - pl.col('room_coor_y').shift(1))\n",
    "                             .over(['session_id']) ** 2).sqrt().alias('room_dist'),\n",
    "                          (pl.col('screen_coor_x') - pl.col('screen_coor_x').shift(1))\n",
    "                             .over(['session_id']).abs().alias('screen_coor_x_diff'),\n",
    "                          (pl.col('screen_coor_y') - pl.col('screen_coor_y').shift(1))\n",
    "                             .over(['session_id']).abs().alias('screen_coor_y_diff'),\n",
    "                            ((pl.col('screen_coor_x') - pl.col('screen_coor_x').shift(1))\n",
    "                             .over(['session_id']) ** 2 + (pl.col('screen_coor_y') - pl.col('screen_coor_y').shift(1))\n",
    "                             .over(['session_id']) ** 2).sqrt().alias('screen_dist'),\n",
    "                    pl.col('index').count().over(['session_id', 'level_group']).alias('index_cnt'),\n",
    "                     (pl.col('elapsed_time').max() - pl.col('elapsed_time').min()).over(['session_id', 'level_group']).alias('elapsed_time_range'),\n",
    "                    ((pl.col('index') - pl.col('index').min()) / (pl.col('index').max() - pl.col('index').min())).over(['session_id', 'level_group']).alias('index2'),\n",
    "                          ((pl.col('elapsed_time') - pl.col('elapsed_time').min()) / (pl.col('elapsed_time').max() - pl.col('elapsed_time').min())).over(['session_id', 'level_group']).alias('elapsed_time2'),\n",
    "                \n",
    "                    ])\n",
    "    num_cols = ['index', 'index2', 'elapsed_time', 'elapsed_time2', 'elapsed_time_diff', 'page',  'room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y', 'hover_duration'] + ['fullscreen','hq','music'] + ['room_dist', 'screen_dist'] + ['room_coor_x_diff', 'room_coor_y_diff'] + ['screen_coor_x_diff', 'screen_coor_y_diff']\n",
    "    cat_cols = ['level', 'level_group', 'event_name', 'name', 'room_fqid', 'fqid', 'text_fqid', 'text']\n",
    "    \n",
    "         \n",
    "    for col in ['event_name', 'name', 'room_fqid', 'fqid', 'text_fqid', 'text']:\n",
    "        df = df.with_columns([(pl.col('elapsed_time') - pl.col('elapsed_time').shift(1)).over(['session_id',col]).alias('elapsed_time_diff_{}'.format(col)),\n",
    "                          (pl.col('room_coor_x') - pl.col('room_coor_x').shift(1))\n",
    "                             .over(['session_id',col]).abs().alias('room_coor_x_diff_{}'.format(col)),\n",
    "                          (pl.col('room_coor_y') - pl.col('room_coor_y').shift(1))\n",
    "                             .over(['session_id',col]).abs().alias('room_coor_y_diff_{}'.format(col)),\n",
    "                           ((pl.col('room_coor_x') - pl.col('room_coor_x').shift(1))\n",
    "                             .over(['session_id',col]) ** 2 + (pl.col('room_coor_y') - pl.col('room_coor_y').shift(1))\n",
    "                             .over(['session_id',col]) ** 2).sqrt().alias('room_dist_{}'.format(col)),\n",
    "                          (pl.col('screen_coor_x') - pl.col('screen_coor_x').shift(1))\n",
    "                             .over(['session_id',col]).abs().alias('screen_coor_x_diff_{}'.format(col)),\n",
    "                          (pl.col('screen_coor_y') - pl.col('screen_coor_y').shift(1))\n",
    "                             .over(['session_id',col]).abs().alias('screen_coor_y_diff_{}'.format(col)),\n",
    "                            ((pl.col('screen_coor_x') - pl.col('screen_coor_x').shift(1))\n",
    "                             .over(['session_id',col]) ** 2 + (pl.col('screen_coor_y') - pl.col('screen_coor_y').shift(1))\n",
    "                             .over(['session_id',col]) ** 2).sqrt().alias('screen_dist_{}'.format(col))])\n",
    "               \n",
    "    all_aggs = []\n",
    "    for col in ['event_name', 'name', 'room_fqid', 'fqid', 'text_fqid', 'text']:\n",
    "        all_aggs += [pl.col(col).n_unique().over(['session_id', 'level_group']).alias('{}_uni'.format(col)),\n",
    "                    pl.col(col).n_unique().over(['session_id', 'level']).alias('{}_uni2'.format(col)),\n",
    "                    pl.col('elapsed_time').rank('dense').over(['session_id', 'level_group', col]).alias('{}_elapsed_time_rank'.format(col)),\n",
    "                    (pl.col('elapsed_time').rank('dense').over(['session_id', 'level_group', col]) / pl.col('elapsed_time').count().over(['session_id', 'level_group', col])).alias('{}_elapsed_time_rank2'.format(col)),\n",
    "                    \n",
    "                    ]\n",
    "        \n",
    "    for col in ['elapsed_time_diff']:\n",
    "        all_aggs += [pl.col(col).std().over(['session_id', 'level_group']).alias('{}_std'.format(col)),\n",
    "                    pl.col(col).mean().over(['session_id', 'level_group']).alias('{}_mean'.format(col)),\n",
    "                    pl.col(col).max().over(['session_id', 'level_group']).alias('{}_max'.format(col)),\n",
    "                    pl.col(col).min().over(['session_id', 'level_group']).alias('{}_min'.format(col)),\n",
    "                    pl.col(col).rank('min').over(['session_id', 'level_group']).alias('{}_rank'.format(col)),\n",
    "                    (pl.col(col).rank('min').over(['session_id', 'level_group']) / pl.col(col).count().over(['session_id', 'level_group'])).alias('{}_rank2'.format(col)),\n",
    "                    pl.col(col).n_unique().over(['session_id', 'level_group']).alias('{}_uni'.format(col)),]\n",
    "        all_aggs += [pl.col(col).quantile(p, \"nearest\").over(['session_id', 'level_group']).alias('{}_p{}'.format(col,p)) for p in [0.1, 0.2, 0.3, 0.4, 0.5,0.6, 0.7, 0.8, 0.9]]\n",
    "        \n",
    "        \n",
    "    for col in ['elapsed_time_diff']:\n",
    "        for col2 in ['level', 'event_name', 'name', 'room_fqid', 'fqid', 'text_fqid', 'text']:\n",
    "            all_aggs += [pl.col(col).std().over(['session_id', 'level_group',col2]).alias('{}_std_col_{}'.format(col, col2)),\n",
    "                    pl.col(col).mean().over(['session_id', 'level_group',col2]).alias('{}_mean_col_{}'.format(col, col2)),\n",
    "                    pl.col(col).max().over(['session_id', 'level_group',col2]).alias('{}_max_col_{}'.format(col, col2)),\n",
    "                    pl.col(col).min().over(['session_id', 'level_group',col2]).alias('{}_min_col_{}'.format(col, col2)),\n",
    "                    pl.col(col).rank('min').over(['session_id', 'level_group',col2]).alias('{}_rank_col_{}'.format(col, col2)),\n",
    "                    (pl.col(col).rank('min').over(['session_id', 'level_group',col2]) / pl.col(col).count().over(['session_id', 'level_group',col2])).alias('{}_rank2_col_{}'.format(col, col2)),\n",
    "                    pl.col(col).count().over(['session_id', 'level_group',col2]).alias('{}_cnt_col_{}'.format(col, col2)),\n",
    "                    pl.col(col).n_unique().over(['session_id', 'level_group',col2]).alias('{}_uni_col_{}'.format(col, col2)),]\n",
    "            all_aggs += [pl.col(col).quantile(p, \"nearest\").over(['session_id', 'level_group', col2]).alias('{}_{}_p{}'.format(col,col2, p)) for p in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]]\n",
    "            \n",
    "\n",
    "    df = df.with_columns(all_aggs)\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def cat_encoder(df, cat_dict):\n",
    "    # Categorical columns to encode\n",
    "    cat_cols = ['level', 'event_name', 'name', 'room_fqid', 'fqid', 'text_fqid', 'text']\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        if col in ['level']:\n",
    "            # If the column is 'level', fill missing values with 0 and convert to int\n",
    "            df[col] = df[col].fillna(0).astype(int)\n",
    "            continue\n",
    "        \n",
    "        # For other columns, convert values to strings, map them using the provided dictionary, fill missing values with 0, and convert to int\n",
    "        df[col] = df[col].astype(str).map(cat_dict[col]).fillna(0).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize(df_np, all_cols, standard_dict):\n",
    "    # Replace infinite values with NaN\n",
    "    df_np[np.isinf(df_np)] = np.nan\n",
    "        \n",
    "    def standard_each(v1, f, mode=1):\n",
    "        # Standardize a single value using the provided mean and standard deviation\n",
    "        df_mean, df_std = f[0], f[1]\n",
    "        v1 = (v1 - df_mean) / df_std\n",
    "        v1 = np.clip(v1, -30, 30)  # Clip the standardized value between -30 and 30\n",
    "        return v1\n",
    "    \n",
    "    # Parallelize the standardization process across multiple columns using joblib\n",
    "    res = Parallel(n_jobs=4)(delayed(standard_each)(df_np[:, i], standard_dict[all_cols[i]]) for i in range(len(all_cols)))\n",
    "    \n",
    "    # Concatenate the standardized values into a single numpy array\n",
    "    df_np = np.concatenate([x.reshape((-1, 1)) for x in res], axis=1)\n",
    "    \n",
    "    # Replace NaN values with 0.0\n",
    "    df_np[np.isnan(df_np)] = 0.0\n",
    "    \n",
    "    return df_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb9d8d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-26T06:56:22.652018Z",
     "iopub.status.busy": "2023-06-26T06:56:22.651406Z",
     "iopub.status.idle": "2023-06-26T06:56:22.741703Z",
     "shell.execute_reply": "2023-06-26T06:56:22.740771Z"
    },
    "papermill": {
     "duration": 0.099305,
     "end_time": "2023-06-26T06:56:22.743952",
     "exception": false,
     "start_time": "2023-06-26T06:56:22.644647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "l1_texts = ['undefined', 'Whatcha doing over there, Jo?', 'Just talking to Teddy.', 'I gotta run to my meeting!', 'Can I come, Gramps?', 'Sure thing, Jo. Grab your notebook and come upstairs!', 'See you later, Teddy.', \"I get to go to Gramps's meeting!\", 'Now where did I put my notebook?', '\\\\u00f0\\\\u0178\\\\u02dc\\\\u00b4', 'null', 'I love these photos of me and Teddy!', 'Found it!', 'Gramps is in trouble for losing papers?', \"This can't be right!\", 'Gramps is a great historian!', \"Hmm. Button's still not working.\", \"Let's get started. The Wisconsin Wonders exhibit opens tomorrow!\", 'Who wants to investigate the shirt artifact?', \"Not Leopold here. He's been losing papers lately.\", 'Hey!', \"It's true, they do keep going missing lately.\", 'See?', 'Besides, I already figured out the shirt.', \"It's a women's basketball jersey!\", 'That settles it.', 'Wells, finish up your report.', \"Leopold, why don't you help me set up in the Capitol?\", 'We need to talk about that missing paperwork.', 'Will do, Boss.', \"Hey Jo, let's take a look at the shirt!\", 'Your grampa is waiting for you in the collection room.', \"Why don't you go catch up with your grampa?\", 'What a fascinating artifact!', \"Wow, that's so cool, Gramps!\", 'Can I take a closer look?', \"Hmmm. Shouldn't you be doing your homework?\", \"It's already all done!\", 'Plus, my teacher said I could help you out for extra credit!', \"Well, that's good enough for me.\", 'Go ahead, take a peek at the shirt!', 'This looks like a clue!', \"I'll record this in my notebook.\", 'Find anything?', 'Yes! This old slip from 1916.', 'I knew it!', \"I'm not so sure that this is a basketball jersey.\", 'Wait, you mean Wells is wrong?!', 'Could be. But we need evidence!', \"Why don't you head to the Basketball Center and rustle up some clues?\", 'Sure!', \"I'll be at the Capitol. Let me know if you find anything!\", 'Better check back later.', \"That's it!\", \"The slip is from 1916 but the team didn't start until 1974!\", 'Our shirt is too old to be a basketball jersey!', 'I need to get to the Capitol and tell Gramps!', 'I should see what Grampa is up to!', 'Ugh. Meetings are so boring.', 'Grab your notebook and come upstairs!', 'Hang tight, Teddy.', \"I'll hurry back and then we can go exploring!\", 'Well, Leopold here is always losing papers...', 'Ha. Told you so!', 'Can we hurry up, Gramps?', 'Teddy and I were gonna go climb that huge tree out back!', \"Hmmm. Don't forget about your homework.\", 'Your teacher said you missed 7 assignments in a row!', 'So? History is boring!', 'I suppose historians are boring, too?', \"No way, Gramps. You're the best!\", 'Then do it for me!', 'Your teacher said you could help me for extra credit.', 'A boring old shirt.', 'Just this old slip from 1916.', 'Do I have to?', 'Um... what did you want me to do again?', 'Head over to the Basketball Center.', 'Hopefully you can find some clues!', 'Meetings are BORING!', \"I feel like I'm forgetting something.\", 'Gramps is the best historian ever!', 'This button never works!', \"Why don't you go play with your grampa?\", \"Look at that! It's the bee's knees!\", \"Well, I did SOME of those. I just couldn't find them!\", 'Did you do all of them?', 'No... because history is boring!', 'Hooray, a boring old shirt.', 'Hot Dog! I knew it!', 'Ooh, I like clues!', 'Hopefully you can rustle up some clues!', 'I should go talk to Gramps!', 'Yes! This cool old slip from 1916.', 'I should see what Gramps is up to!', 'Gramps said to look for clues. Better look around.', 'Have a look at the artifact!', 'Come on, Jo!', \"Meet me back in my office and we'll get started!\"]\n",
    "l2_texts = ['undefined', 'Whatcha doing over there, Jo?', 'Just talking to Teddy.', 'I gotta run to my meeting!', 'Can I come, Gramps?', 'Sure thing, Jo. Grab your notebook and come upstairs!', 'See you later, Teddy.', \"I get to go to Gramps's meeting!\", 'Now where did I put my notebook?', '\\\\u00f0\\\\u0178\\\\u02dc\\\\u00b4', 'null', 'I love these photos of me and Teddy!', 'Found it!', 'Gramps is in trouble for losing papers?', \"This can't be right!\", 'Gramps is a great historian!', \"Hmm. Button's still not working.\", \"Let's get started. The Wisconsin Wonders exhibit opens tomorrow!\", 'Who wants to investigate the shirt artifact?', \"Not Leopold here. He's been losing papers lately.\", 'Hey!', \"It's true, they do keep going missing lately.\", 'See?', 'Besides, I already figured out the shirt.', \"It's a women's basketball jersey!\", 'That settles it.', 'Wells, finish up your report.', \"Leopold, why don't you help me set up in the Capitol?\", 'We need to talk about that missing paperwork.', 'Will do, Boss.', \"Hey Jo, let's take a look at the shirt!\", 'Your grampa is waiting for you in the collection room.', \"Why don't you go catch up with your grampa?\", 'What a fascinating artifact!', \"Wow, that's so cool, Gramps!\", 'Can I take a closer look?', \"Hmmm. Shouldn't you be doing your homework?\", \"It's already all done!\", 'Plus, my teacher said I could help you out for extra credit!', \"Well, that's good enough for me.\", 'Go ahead, take a peek at the shirt!', 'This looks like a clue!', \"I'll record this in my notebook.\", 'Find anything?', 'Yes! This old slip from 1916.', 'I knew it!', \"I'm not so sure that this is a basketball jersey.\", 'Wait, you mean Wells is wrong?!', 'Could be. But we need evidence!', \"Why don't you head to the Basketball Center and rustle up some clues?\", 'Sure!', \"I'll be at the Capitol. Let me know if you find anything!\", 'Better check back later.', \"That's it!\", \"The slip is from 1916 but the team didn't start until 1974!\", 'Our shirt is too old to be a basketball jersey!', 'I need to get to the Capitol and tell Gramps!', 'What are you still doing here,  Jolie?', 'Go find your grampa and get to work!', 'Oh no!', 'What happened here?!', \"I don't know!\", 'I got here and the whole place was a mess!', 'Can you help me tidy up?', \"Teddy's scarf! Somebody must've taken him!\", 'Try not to panic, Jo.', 'Maybe he just got scared and ran off.', 'But he never goes anywhere without his scarf!', \"I think he's in trouble!\", 'Is this your coffee, Gramps?', \"Nope, that's from Bean Town. I only drink Holdgers!\", \"Who could've done this?\", \"It must've been Wells.\", \"He's always trying to get you in trouble, and he doesn't like animals!\", 'Slow down, Jo.', 'But what if Wells kidnapped Teddy?', 'Then we need evidence.', \"You're right, Gramps. Let's investigate!\", \"I'm afraid my papers have gone missing in this mess.\", \"You'll have to get started without me.\", \"Okay. I'll find Teddy!\", \"And I'll figure out the shirt, too.\", 'I knew I could count on you, Jo!', \"Why don't you go upstairs and see the archivist?\", \"He's our expert record keeper.\", 'I need your help!', 'Who are you?', \"I'm Leopold's grandkid!\", \"Sorry, I'm too busy for kids right now.\", 'Now if only I could read this thing.', \"Can't believe I lost my reading glasses.\", 'I bet the archivist could use this!', \"Ah, that's better!\", 'Did you have a question?', 'Yes! I was wondering-', 'Wait a minute!', 'Where did you get that coffee?', \"Oh, that's from Bean Town.\", 'I ran into Wells there this morning.', 'Wells? I knew it!', 'Do you know anything about this slip?', 'I found it on an old shirt.', 'An old shirt? Try the university.', 'You can talk to a textile expert there.', \"What's a textile expert?\", 'They study clothes and fabric.', 'Great! Thanks for the help!', 'Head over to the university.', 'Hello there!', 'Wow! What is all this stuff?', \"It's our Norwegian Craft exhibit!\", 'Can I give you the tour?', \"Sorry, I'm in a hurry.\", 'Do you know what this slip is?', 'Looks like a dry cleaning receipt.', 'Thanks.', 'Now I Just need to find all the cleaners from way back in 1916.', 'Maybe I can help!', \"I've got a stack of business cards from my favorite cleaners.\", \"Why don't you take a look?\", 'This place was around in 1916! I can start there!', \"You haven't seen any badgers around here, have you?\", 'Badgers? No.', 'Okay. Thanks anyway.', 'Hi! How can I help you?', 'I need to find the owner of this slip.', \"Well, I can't show our log books to just anybody.\", 'Please?', \"It's for Grampa Leo. He's a historian!\", 'Leo... you mean Leopold?', 'Your gramps is awesome! Always full of stories.', \"Guess it couldn't hurt to let you take a look.\", \"Here's the log book.\", \"It's a match!\", 'Theodora Youmans must be the owner!', 'Do you know who Theodora Youmans is?', \"Hmmm... not sure. Why don't you try the library?\", 'Thanks for the help!', 'Oh, hello there!', 'How can I help you?', 'Have you seen a badger around here?', \"I'm afraid not.\", 'Please let me know if you do.', \"I'm also looking for Theodora Youmans. Have you heard of her?\", 'Theodora Youmans? Of course!', \"Check out our microfiche. It's right through that door.\", 'Youmans was a suffragist!', 'She helped get votes for women!', 'Wells! What was he doing here? I should ask the librarian.', 'What was Wells doing here?', 'He was looking for a taxidermist.', \"What's a taxidermist?\", 'Not sure. Here, let me look it up.', '\\\\Taxidermy: the art of preparing, stuffing, and mounting the skins of animals.\\\\', 'Oh no... Teddy!', 'Can you help me find Wells?', 'You could ask the archivist. He knows everybody!', \"Jolie! I was hoping you'd stop by. Any news on the shirt artifact?\", \"I haven't quite figured it out just yet...\", \"Well, get on it. I'm counting on you and your gramps to figure this out!\", 'Can you help me? I need to find Wells!', \"I haven't seen him.\", 'Please? This is really important.', \"Sorry, can't help you.\", 'Do you have any info on Theodora Youmans?', 'Theodora Youmans? Is that who owned the shirt?', 'Yep.', \"Why didn't you say so?\", 'Youmans was a suffragist here in Wisconsin.', 'She led marches and helped women get the right to vote!', \"Wait a sec. Women couldn't vote?!\", 'Nope. But Youmans and other suffragists worked hard to change that.', 'Thanks to them, Wisconsin was the first state to approve votes for women!', 'Wow!', \"Here's a call number to find more info in the Stacks.\", 'Where are the Stacks?', 'Right outside the door.', 'Hey, this is Youmans!', \"And look! She's wearing the shirt!\", 'I should go to the Capitol and tell everyone!', 'I should see what Grampa is up to!', 'What should I do first?', 'Head upstairs and talk to the archivist. He might be able to help!', 'Ugh. Meetings are so boring.', 'Grab your notebook and come upstairs!', 'Hang tight, Teddy.', \"I'll hurry back and then we can go exploring!\", 'Well, Leopold here is always losing papers...', 'Ha. Told you so!', 'Can we hurry up, Gramps?', 'Teddy and I were gonna go climb that huge tree out back!', \"Hmmm. Don't forget about your homework.\", 'Your teacher said you missed 7 assignments in a row!', 'So? History is boring!', 'I suppose historians are boring, too?', \"No way, Gramps. You're the best!\", 'Then do it for me!', 'Your teacher said you could help me for extra credit.', 'A boring old shirt.', 'Just this old slip from 1916.', 'Do I have to?', 'What the-', 'I have an idea.', \"He's wrong about old shirts and his name rhymes with \\\\smells\\\\...\", 'BUT WELLS STOLE TEDDY!', 'Could be. But we need evidence.', \"Fine. Let's investigate!\", \"Don't worry, Gramps. I'll find Teddy!\", \"Please let me know if you do. It's important!\", 'I need to find Wells right away! Do you know where he is?', 'I need to find Wells!!!', \"I can't calm down. This is important!\", 'Ugh. Fine.', 'Um... what did you want me to do again?', 'Head over to the Basketball Center.', 'Hopefully you can find some clues!', 'I love these photos of me and Teddy.', 'I should stay and look for clues!', 'Where should I go again?', 'You could try the archivist. Maybe he can help you find Wells!', 'Hi, Mrs. M.', \"I don't need that right now.\", 'Meetings are BORING!', \"I feel like I'm forgetting something.\", 'Gramps is the best historian ever!', 'This button never works!', \"Why don't you go play with your grampa?\", \"Look at that! It's the bee's knees!\", \"Well, I did SOME of those. I just couldn't find them!\", 'Did you do all of them?', 'No... because history is boring!', 'Hooray, a boring old shirt.', 'Hot Dog! I knew it!', 'Ooh, I like clues!', 'Hopefully you can rustle up some clues!', 'I got here and the whole place was ransacked!', 'Hold your horses, Jo.', '*grumble grumble*', 'And you are?', \"I don't have time for kids.\", 'Now if only I could read this thing. Blasted tiny letters...', 'Knew what?', 'Did you have a question or not?', 'Yes!', \"You're still here? I'm trying to work!\", 'Run along to the university.', 'Ooh, thanks!', 'Now I just need to find all the cleaners from wayyyy back in 1916.', 'Yikes... this could take a while.', 'Hi! *cough*', 'Can you help-', '*cough cough*', 'Can you help me-', '*COUGH COUGH COUGH*', 'Um, are you okay?', \"Oh, I'm fine! Just a little hoarse.\", 'Ha! What do you call a pony with a sore throat?', 'Huh?', 'A little horse!', \"Ha! You're funny.\", 'I got that one from my Gramps!', 'Can you help me? I need to find the owner of this slip.', \"Yup, that's him!\", \"Unless you're too busy horsing around.\", 'Ha! Good one.', \"You look like you're on a mission.\", 'Two missions, actually!', 'Oh my!', 'I need to find Wells right away!! Do you know where he is?', \"Calm down, kid. I haven't seen him.\", 'I should help Gramps clean.', \"Maybe there's a clue in this mess!\", \"Poor Gramps! I should make sure he's okay.\", 'The archivist said I should look in the stacks.', 'I should go talk to Gramps!', 'Yeah. Thanks anyway.', 'What are you waiting for? The Stacks are right outside the door.', 'Yes! This cool old slip from 1916.', 'Are you okay?', \"Weren't you going to check out our microfiche?\", \"I'm sure you'll find Theodora in there somewhere!\", 'Well? What are you still doing here?', 'So much cleaning to do...', 'I used to have a magnifying glass around here\\\\u00e2\\\\u20ac\\\\u00a6', \"But I hear the museum's got one on the loose!\", \"Did you drop something, Dear? There's a card on the floor.\", 'Take a look!', \"It's such a nice fall day.\", 'I should see what Gramps is up to!', 'I found it!', 'Theodora wearing the shirt!', 'You better get to the capitol!', 'Nice decorations.', 'Nice seeing you, Jolie!', 'Did you drop something, Dear?', 'Gramps said to look for clues. Better look around.', 'I should find out if she can help me!', 'Ooh, nice decorations!', 'The libarian said I could find some information on Youmans in here...', 'Have a look at the artifact!', 'I should ask the librarian why Wells was here.', \"I wonder if there's a clue in those business cards...\", 'Thanks. Did you figure out the shirt?', 'Welcome back, Jolie. Did you figure out the shirt?', 'I should check that logbook to see who owned this slip...', 'AND I know who took Teddy!', 'Who is Teddy?', \"And where's your grampa?\", 'Sorry for the delay, Boss.', 'I had some cleaning up to do in my office.', 'Mrs. M, I think Wells kidnapped Teddy.', \"And he messed up Gramps's office, too!\", 'One step at a time, Jo.', 'Did you figure out the shirt?', 'I knew you could do it, Jo!', 'Now can I tell you what happened to Teddy?', 'He needs our help!', \"Sorry I'm late.\", \"Wells! Where's Teddy? Is he okay?\", 'I figured out that you kidnapped him!', 'Easy, Jo.', \"Why don't you prove your case?\", \"It'll be okay, Jo. We'll find Teddy!\", 'Nice work on the shirt, Jolie!', 'Leopold, can you run back to the museum?', 'Sounds good, Boss.', 'Jo, meet me back at my office.', 'I hope you find your badger, kid.', 'Come on, Jo!', \"Meet me back in my office and we'll get started!\", 'Here I am!', 'Wells sabotaged Gramps!', 'AND he stole Teddy!']\n",
    "l3_texts = ['undefined', 'Whatcha doing over there, Jo?', 'Just talking to Teddy.', 'I gotta run to my meeting!', 'Can I come, Gramps?', 'Sure thing, Jo. Grab your notebook and come upstairs!', 'See you later, Teddy.', \"I get to go to Gramps's meeting!\", 'Now where did I put my notebook?', '\\\\u00f0\\\\u0178\\\\u02dc\\\\u00b4', 'null', 'I love these photos of me and Teddy!', 'Found it!', 'Gramps is in trouble for losing papers?', \"This can't be right!\", 'Gramps is a great historian!', \"Hmm. Button's still not working.\", \"Let's get started. The Wisconsin Wonders exhibit opens tomorrow!\", 'Who wants to investigate the shirt artifact?', \"Not Leopold here. He's been losing papers lately.\", 'Hey!', \"It's true, they do keep going missing lately.\", 'See?', 'Besides, I already figured out the shirt.', \"It's a women's basketball jersey!\", 'That settles it.', 'Wells, finish up your report.', \"Leopold, why don't you help me set up in the Capitol?\", 'We need to talk about that missing paperwork.', 'Will do, Boss.', \"Hey Jo, let's take a look at the shirt!\", 'Your grampa is waiting for you in the collection room.', \"Why don't you go catch up with your grampa?\", 'What a fascinating artifact!', \"Wow, that's so cool, Gramps!\", 'Can I take a closer look?', \"Hmmm. Shouldn't you be doing your homework?\", \"It's already all done!\", 'Plus, my teacher said I could help you out for extra credit!', \"Well, that's good enough for me.\", 'Go ahead, take a peek at the shirt!', 'This looks like a clue!', \"I'll record this in my notebook.\", 'Find anything?', 'Yes! This old slip from 1916.', 'I knew it!', \"I'm not so sure that this is a basketball jersey.\", 'Wait, you mean Wells is wrong?!', 'Could be. But we need evidence!', \"Why don't you head to the Basketball Center and rustle up some clues?\", 'Sure!', \"I'll be at the Capitol. Let me know if you find anything!\", 'Better check back later.', \"That's it!\", \"The slip is from 1916 but the team didn't start until 1974!\", 'Our shirt is too old to be a basketball jersey!', 'I need to get to the Capitol and tell Gramps!', 'What are you still doing here,  Jolie?', 'Go find your grampa and get to work!', 'Oh no!', 'What happened here?!', \"I don't know!\", 'I got here and the whole place was a mess!', 'Can you help me tidy up?', \"Teddy's scarf! Somebody must've taken him!\", 'Try not to panic, Jo.', 'Maybe he just got scared and ran off.', 'But he never goes anywhere without his scarf!', \"I think he's in trouble!\", 'Is this your coffee, Gramps?', \"Nope, that's from Bean Town. I only drink Holdgers!\", \"Who could've done this?\", \"It must've been Wells.\", \"He's always trying to get you in trouble, and he doesn't like animals!\", 'Slow down, Jo.', 'But what if Wells kidnapped Teddy?', 'Then we need evidence.', \"You're right, Gramps. Let's investigate!\", \"I'm afraid my papers have gone missing in this mess.\", \"You'll have to get started without me.\", \"Okay. I'll find Teddy!\", \"And I'll figure out the shirt, too.\", 'I knew I could count on you, Jo!', \"Why don't you go upstairs and see the archivist?\", \"He's our expert record keeper.\", 'I need your help!', 'Who are you?', \"I'm Leopold's grandkid!\", \"Sorry, I'm too busy for kids right now.\", 'Now if only I could read this thing.', \"Can't believe I lost my reading glasses.\", 'I bet the archivist could use this!', \"Ah, that's better!\", 'Did you have a question?', 'Yes! I was wondering-', 'Wait a minute!', 'Where did you get that coffee?', \"Oh, that's from Bean Town.\", 'I ran into Wells there this morning.', 'Wells? I knew it!', 'Do you know anything about this slip?', 'I found it on an old shirt.', 'An old shirt? Try the university.', 'You can talk to a textile expert there.', \"What's a textile expert?\", 'They study clothes and fabric.', 'Great! Thanks for the help!', 'Head over to the university.', 'Hello there!', 'Wow! What is all this stuff?', \"It's our Norwegian Craft exhibit!\", 'Can I give you the tour?', \"Sorry, I'm in a hurry.\", 'Do you know what this slip is?', 'Looks like a dry cleaning receipt.', 'Thanks.', 'Now I Just need to find all the cleaners from way back in 1916.', 'Maybe I can help!', \"I've got a stack of business cards from my favorite cleaners.\", \"Why don't you take a look?\", 'This place was around in 1916! I can start there!', \"You haven't seen any badgers around here, have you?\", 'Badgers? No.', 'Okay. Thanks anyway.', 'Hi! How can I help you?', 'I need to find the owner of this slip.', \"Well, I can't show our log books to just anybody.\", 'Please?', \"It's for Grampa Leo. He's a historian!\", 'Leo... you mean Leopold?', 'Your gramps is awesome! Always full of stories.', \"Guess it couldn't hurt to let you take a look.\", \"Here's the log book.\", \"It's a match!\", 'Theodora Youmans must be the owner!', 'Do you know who Theodora Youmans is?', \"Hmmm... not sure. Why don't you try the library?\", 'Thanks for the help!', 'Oh, hello there!', 'How can I help you?', 'Have you seen a badger around here?', \"I'm afraid not.\", 'Please let me know if you do.', \"I'm also looking for Theodora Youmans. Have you heard of her?\", 'Theodora Youmans? Of course!', \"Check out our microfiche. It's right through that door.\", 'Youmans was a suffragist!', 'She helped get votes for women!', 'Wells! What was he doing here? I should ask the librarian.', 'What was Wells doing here?', 'He was looking for a taxidermist.', \"What's a taxidermist?\", 'Not sure. Here, let me look it up.', '\\\\Taxidermy: the art of preparing, stuffing, and mounting the skins of animals.\\\\', 'Oh no... Teddy!', 'Can you help me find Wells?', 'You could ask the archivist. He knows everybody!', \"Jolie! I was hoping you'd stop by. Any news on the shirt artifact?\", \"I haven't quite figured it out just yet...\", \"Well, get on it. I'm counting on you and your gramps to figure this out!\", 'Can you help me? I need to find Wells!', \"I haven't seen him.\", 'Please? This is really important.', \"Sorry, can't help you.\", 'Do you have any info on Theodora Youmans?', 'Theodora Youmans? Is that who owned the shirt?', 'Yep.', \"Why didn't you say so?\", 'Youmans was a suffragist here in Wisconsin.', 'She led marches and helped women get the right to vote!', \"Wait a sec. Women couldn't vote?!\", 'Nope. But Youmans and other suffragists worked hard to change that.', 'Thanks to them, Wisconsin was the first state to approve votes for women!', 'Wow!', \"Here's a call number to find more info in the Stacks.\", 'Where are the Stacks?', 'Right outside the door.', 'Hey, this is Youmans!', \"And look! She's wearing the shirt!\", 'I should go to the Capitol and tell everyone!', 'Jo!', 'Check out the next artifact!', 'What is it?', \"I think it's a flag! Pretty interesting, huh?\", \"It's really cool, Gramps. But I'm worried about Teddy.\", \"He's still missing!\", \"We'll find him, Jo.\", 'Want to look for more clues?', \"We'll find Teddy.\", 'We just have to keep our eyes open!', 'Hey, look at those scratches!', 'The kidnapper probably took Teddy on the elevator!', \"You're right, Jo!\", \"Why isn't the button working?\", \"We'll need a key card.\", 'I had one, but Teddy chewed it up.', \"I've got Wells's ID!\", 'What should we do next?', \"I need to take the artifact upstairs. Why don't you investigate those scratch marks?\", \"Okay. I'll try.\", 'Teddy, here I come!', 'I wonder whose glasses these are.', 'Teddy!!!', \"Hang on. I'll get you out of there!\", 'Whoever lost these glasses probably took Teddy!', 'How can I find out whose glasses these are?', 'Oh! There was a staff directory in the entryway!', \"I'll go look at everyone's pictures!\", 'Those are the same glasses!', \"The archivist must've taken Teddy!\", \"Yes! It's the key for Teddy's cage!\", 'I found the key!', \"Come on, let's get out of here!\", \"Here's your scarf back!\", 'What are you doing down here?', 'And how did that badger get free?', \"I'm here to rescue my friend!\", \"What's going on here?\", 'Thanks for coming, Boss.', 'I told you!', 'I captured a badger in our museum!', \"He's been eating my lunch every day this week!\", 'He has??', \"I've seen him eating homework and important papers, too.\", \"Jolie- keep your badger under control, or he'll have to go.\", 'And you, Frank-', \"You can't just steal Jolie's pet.\", 'Ugh. Fine.', 'Alright, Jolie. Back to work.', \"Come on, Teddy. Let's go help Gramps!\", \"Let's go help Gramps!\", 'Gramps must be up in the collection room.', \"Let's go find him!\", \"Teddy! I'm glad to see you.\", 'The archivist had him locked up!', 'Poor badger.', \"You're becoming quite the detective, Jo.\", 'Notice any clues about this flag?', 'Well... it looks hand-stitched.', 'Good catch!', 'Go on, tell the boss what you found!', \"I'm telling you, Boss. Taxidermy is the way to go!\", 'Nonsense. I want live animals at the exhibit, not stuffed ones.', \"Ah, Jolie! I'm glad you're here.\", \"I'm putting you in charge of the flag case.\", 'Make sure to get some old photos for the exhibit, like last time!', \"Wait! Can't I do it?\", 'The symbol on the flag looks sort of like a deer hoof.', 'It could be an early design for the Wisconsin state flag!', 'Wells, you already have a job to do.', 'What now, kid?', 'Do you really think that symbol is a deer hoof?', 'Not sure.', 'Do you know where I can find a deer expert?', 'Hmm. You could try the Aldo Leopold Wildlife Center.', 'I have to head over there and check out the animals.', \"I'll ride with you!\", \"Come on, kid. Let's go.\", 'Head over to the Wildlife Center!', \"I'm sure they'll be able to help.\", 'People sure drink a lot of coffee around here.', \"I can't believe this.\", 'Ugh...', 'Oh no! What happened to that crane?', 'Her beak is stuck in a coffee cup.', \"It's lucky we found her.\", 'Ugh! Those cups are all over the place.', \"I need to get her free. She won't hold still!\", 'Can Teddy and I help?', 'Sure! Give it a try.', 'Careful. That beak is sharp!', 'We need to calm her down, Teddy.', 'Any ideas?', '\\\\u00f0\\\\u0178\\\\u00a6\\\\u2014', 'Oh yeah, cranes eat insects!', 'Luckily there are tons of insects around here...', 'Got one!', \"Maybe she'll let me take off the cup!\", \"It's OK, girl! Look, I found you a cricket!\", 'You did it! Thanks, kid.', 'Can I help you with anything?', \"I'm investigating this symbol.\", 'Does it look like a deer hoof?', \"There's a diagram of animal tracks over there.\", 'Go take a look!', \"That hoofprint doesn't match the flag!\", 'Thanks for your help, kid!', \"So? What'd you find out?\", \"Looks like it's not a deer hoof.\", \"Oh no. If I don't impress the boss soon,  I'm gonna get fired!\", 'Hey, Wells...', 'I think I might be able to help you.', \"No thanks. I don't need help from kids.\", 'Are you sure? I know where you can find a real, live badger for the exhibit!', 'Wait! What?! Really?', 'Wells, meet Teddy.', '\\\\u00f0\\\\u0178\\\\u02dc\\\\u0160', \"He says he'd be willing to help out.\", 'Yes!!!', 'We still need to figure out that flag. Do you know anyone who could help?', \"Hmm. Let's see...\", 'Actually, I went to school with somebody who LOVES old flags.', \"Why don't you go talk to her? I'll let her know you're coming.\", 'Hey, nice dog! What breed is he?', \"Actually, he's a badger.\", \"Oh, cool! I've never seen a badger in real life.\", \"You've got a million flags here!\", \"Yep. I'm a vexillophile!\", \"What's a vexillophile? \", 'It just means flag expert. How can I help?', \"I'm investigating this flag.\", 'Can you take a look?', \"Hey, I've seen that symbol before! Check it out!\", '\\\\Ecology flag, by Ron Cobb.\\\\', \"It's an ecology flag!\", 'Do you know what this flag was used for?', \"I'm not sure.\", \"If I were you, I'd go to the library and do some digging.\", 'Good idea. Thanks!', 'Welcome back, Dear! How can I help you?', 'I need to learn more about this flag!', 'It has something to do with ecology.', 'Hmm... those stripes remind me of the American flag.', 'Your flag must have been part of a national movement!', \"Go check the microfiche. Maybe you'll find something!\", \"Hey! That's Governor Nelson in front of our flag!\", 'I found the flag! Governor Nelson used it on the first Earth Day!', 'Wow! You figured it out!', 'Now I just need some old photos, like last time.', 'The boss is gonna love it!', 'You could try the archives.', 'Though the archivist might be too busy to help...', 'Okay. Thanks!', 'What are you doing here?', \"We're looking for some photos.\", \"It's for the flag display!\", 'Wait a minute...', \"YOU'RE the new history detective everybody's talking about?\", \"Teddy's helping too.\", 'What kind of photos do you need?', 'Something to do with ecology and Wisconsin.', \"Here's a call number for the Stacks. Go find some photos.\", 'Look at all those activists!', 'This is perfect for the exhibit.', 'I should go to the Capitol and tell Mrs. M!', 'I should see what Grampa is up to!', 'What should I do first?', 'Head upstairs and talk to the archivist. He might be able to help!', \"It's locked!\", \"Jolie! I was hoping you'd stop by. Any news on the flag artifact?\", \"Well, get on it. I'm counting on you to figure this out!\", 'Nice seeing you, Jolie!', \"It's such a nice fall day.\", 'I love these photos of me and Teddy.', \"Why don't you go talk to the boss?\", \"She's right outside.\", 'My friend is a flag expert.', 'She should be able to help you out.', 'There are some old newspapers loaded up in the microfiche.', 'The Stacks are right outside the door. Go find some photos!', 'Ugh. Meetings are so boring.', 'Grab your notebook and come upstairs!', 'Hang tight, Teddy.', \"I'll hurry back and then we can go exploring!\", 'Well, Leopold here is always losing papers...', 'Ha. Told you so!', 'Can we hurry up, Gramps?', 'Teddy and I were gonna go climb that huge tree out back!', \"Hmmm. Don't forget about your homework.\", 'Your teacher said you missed 7 assignments in a row!', 'So? History is boring!', 'I suppose historians are boring, too?', \"No way, Gramps. You're the best!\", 'Then do it for me!', 'Your teacher said you could help me for extra credit.', 'A boring old shirt.', 'Just this old slip from 1916.', 'Do I have to?', 'What the-', 'I have an idea.', \"He's wrong about old shirts and his name rhymes with \\\\smells\\\\...\", 'BUT WELLS STOLE TEDDY!', 'Could be. But we need evidence.', \"Fine. Let's investigate!\", \"Don't worry, Gramps. I'll find Teddy!\", \"Please let me know if you do. It's important!\", 'I need to find Wells right away! Do you know where he is?', 'I need to find Wells!!!', \"I can't calm down. This is important!\", \"I don't have time for this, Gramps.\", 'Teddy is still missing!', \"Let's follow those scratch marks!\", \"I can't go with you. I need to take the artifact upstairs.\", \"It's okay, Gramps. I'll go by myself.\", 'You stole Teddy! How could you?!', \"No he hasn't!\", \"Yes, he has. I've seen him eating homework and important papers, too.\", 'Come on, Teddy.', \"Let's go find Gramps!\", 'I think I can help with your animal problem.', \"Ha! I don't need your help.\", \"Fine. Then I guess you don't want a real, live badger for the exhibit.\", \"Oh, trust me. He'll make time.\", 'Um... what did you want me to do again?', 'Head over to the Basketball Center.', 'Hopefully you can find some clues!', 'I should stay and look for clues!', 'Where should I go again?', 'You could try the archivist. Maybe he can help you find Wells!', 'Hi, Mrs. M.', 'Head back to the museum. Your gramps is waiting for you.', \"I don't need that right now.\", 'Meetings are BORING!', \"I feel like I'm forgetting something.\", 'Gramps is the best historian ever!', 'This button never works!', \"Why don't you go play with your grampa?\", \"Look at that! It's the bee's knees!\", \"Well, I did SOME of those. I just couldn't find them!\", 'Did you do all of them?', 'No... because history is boring!', 'Hooray, a boring old shirt.', 'Hot Dog! I knew it!', 'Ooh, I like clues!', 'Hopefully you can rustle up some clues!', 'I got here and the whole place was ransacked!', 'Hold your horses, Jo.', '*grumble grumble*', 'And you are?', \"I don't have time for kids.\", 'Now if only I could read this thing. Blasted tiny letters...', 'Knew what?', 'Did you have a question or not?', 'Yes!', \"You're still here? I'm trying to work!\", 'Run along to the university.', 'Ooh, thanks!', 'Now I just need to find all the cleaners from wayyyy back in 1916.', 'Yikes... this could take a while.', 'Hi! *cough*', 'Can you help-', '*cough cough*', 'Can you help me-', '*COUGH COUGH COUGH*', 'Um, are you okay?', \"Oh, I'm fine! Just a little hoarse.\", 'Ha! What do you call a pony with a sore throat?', 'Huh?', 'A little horse!', \"Ha! You're funny.\", 'I got that one from my Gramps!', 'Can you help me? I need to find the owner of this slip.', \"Yup, that's him!\", \"Unless you're too busy horsing around.\", 'Ha! Good one.', \"You look like you're on a mission.\", 'Two missions, actually!', 'Oh my!', 'I need to find Wells right away!! Do you know where he is?', \"Calm down, kid. I haven't seen him.\", \"I think it's a flag! Pretty spiffy, eh?\", \"Great Scott, you're right!\", \"Jo! I can't go with you. I need to take the artifact upstairs.\", '\\\\u00f0\\\\u0178\\\\u02dc\\\\u00ad', '\\\\u00e2\\\\u009d\\\\u00a4\\\\u00ef\\\\u00b8\\\\u008f', 'GRRRRRRR', 'GAH! And what is THAT doing out of its cage?!', '\\\\u00f0\\\\u0178\\\\u02dc\\\\u0090', 'Teddy! Did you really eat his lunch?', \"Did you steal Gramps's paperwork too?!\", 'And my homework?!?!', 'See?!', \"That thing's a monster!\", \"I don't have time for this.\", 'YEAH!', 'Wait- me?', \"You can't just steal Jolie's pet. Don't you know badgers are protected animals?\", 'Besides, he looks friendly to me.', 'Wha?!', '\\\\u00f0\\\\u0178\\\\u02dc\\\\u009d', \"Teddy! I'm sure glad to see you.\", 'Gadzooks! Poor critter.', 'Aha! Good catch, Jo.', 'Not sure. Do I look like a deer expert to you?', 'Ugh. I have to head over there and check out the animals.', 'FINE. That possum better not scratch my leather seats...', \"He's a badger!\", '\\\\u00f0\\\\u0178\\\\u00a7\\\\u02dc', 'Yoga does sound nice.', \"But cranes can't do yoga, Teddy!\", '\\\\u00f0\\\\u0178\\\\u008d\\\\u00a9', \"Cranes don't eat donuts!\", 'Besides, you just ate my last snack.', \"Gah. I can't believe this.\", \"I'm a historian, not a zookeeper!\", 'And this place is dirty, and itchy, and-', 'I love it!', \"Of course you do. You've got a rodent following you around.\", \"Actually, badgers aren't rodents-\", 'Whatever.', 'Great. Just great. Could this day get any worse?!', \"Yes!!! I'm saved!\", 'A real, live ferret!', \"He's. A. Badger.\", 'And we still need to figure out that flag!', \"Fine, fine. Let's see...\", 'A vexy-wha?', 'Ooh... \\\\Ecology flag, by Ron Cobb.\\\\', 'The boss is gonna love it!!!', \"You again! Don't let him hurt me!\", '\\\\u00f0\\\\u0178\\\\u2122\\\\u201e', \"Actually, we're just here for some photos.\", 'Guess so!', 'YOU?!', \"Just please, don't let your badger eat them!\", 'I should help Gramps clean.', \"Maybe there's a clue in this mess!\", \"Poor Gramps! I should make sure he's okay.\", 'The archivist said I should look in the stacks.', 'There should be some info about that symbol in my book.', 'I should go talk to Gramps!', 'Yeah. Thanks anyway.', 'What are you waiting for? The Stacks are right outside the door.', 'Yes! This cool old slip from 1916.', 'Are you okay?', \"I'll be in the collection room. Come find me when you're ready to check out the artifact.\", 'Good luck!', 'What?!', 'Can I ride with you?', \"Don't worry, he won't! (And he's a badger, by the way.)\", 'Ugh... I think that lynx is looking at me funny.', \"Don't worry, Teddy won't eat your lunch anymore!\", \"We're just looking for photos for the flag display.\", \"Weren't you going to check out our microfiche?\", \"I'm sure you'll find Theodora in there somewhere!\", \"But I hear the museum's got one on the loose!\", 'Well? What are you still doing here?', 'So much cleaning to do...', 'I should check out that pair of glasses.', 'I should ask the librarian where to go next.', \"Check out the archives. They've got tons of old photos!\", 'I used to have a magnifying glass around here\\\\u00e2\\\\u20ac\\\\u00a6', \"Come on, kid. You're slowing me down.\", \"Did you drop something, Dear? There's a card on the floor.\", 'Take a look!', 'I should see what Gramps is up to!', 'I found it!', 'Theodora wearing the shirt!', 'You better get to the capitol!', 'Nice decorations.', 'Did you drop something, Dear?', 'Gramps said to look for clues. Better look around.', 'I should find out if she can help me!', 'Ooh, nice decorations!', 'The libarian said I could find some information on Youmans in here...', 'Have a look at the artifact!', 'What is it, Teddy?', 'Oh no... they got sick from polluted water?', 'Poor foxes!', 'I should ask the librarian why Wells was here.', \"I wonder if there's a clue in those business cards...\", 'Thanks. Did you figure out the shirt?', 'Jolie! Where have you been?', 'The exhibit opens tomorrow.', 'Welcome back, Jolie. Did you figure out the shirt?', 'Wells got in trouble for littering at the Wildlife Center.', 'I should check that logbook to see who owned this slip...', 'AND I know who took Teddy!', 'Who is Teddy?', \"And where's your grampa?\", 'Sorry for the delay, Boss.', 'I had some cleaning up to do in my office.', 'Mrs. M, I think Wells kidnapped Teddy.', \"And he messed up Gramps's office, too!\", 'One step at a time, Jo.', 'Did you figure out the shirt?', 'I knew you could do it, Jo!', 'Now can I tell you what happened to Teddy?', 'He needs our help!', \"Sorry I'm late.\", \"Wells! Where's Teddy? Is he okay?\", 'I figured out that you kidnapped him!', 'Easy, Jo.', \"Why don't you prove your case?\", \"It'll be okay, Jo. We'll find Teddy!\", 'Nice work on the shirt, Jolie!', 'Leopold, can you run back to the museum?', 'Sounds good, Boss.', 'Jo, meet me back at my office.', 'I hope you find your badger, kid.', 'Thanks!', \"Are you going home now? Tomorrow's the big day!\", 'He got a park named after him? Cool!', 'Come on, Jo!', \"Meet me back in my office and we'll get started!\", 'Here I am!', 'Wells sabotaged Gramps!', 'AND he stole Teddy!']\n",
    "\n",
    "l1_texts_map = {j:i for i, j in enumerate(l1_texts)}\n",
    "l2_texts_map = {j:i for i, j in enumerate(l2_texts)}\n",
    "l3_texts_map = {j:i for i, j in enumerate(l3_texts)}\n",
    "\n",
    "texts_map = {'0-4': l1_texts_map, '5-12': l2_texts_map, '13-22': l3_texts_map}\n",
    "\n",
    "def map_level_group(q):\n",
    "    #  takes a parameter q and assigns a level group based on its value. If q is less than 4, it returns '0-4'. If q is between 4 and 13 (inclusive), it returns '5-12'. Otherwise, it returns '13-22'. This function is used to map a question number to a level group.\n",
    "    if q<4:\n",
    "        return '0-4'\n",
    "    elif q>=4 and q<=13:\n",
    "        return '5-12'\n",
    "    else:\n",
    "        return '13-22'\n",
    "    \n",
    "def add_features(df, logs, level_limit, texts_map):\n",
    "    columns = [\n",
    "        (pl.col('elapsed_time') - pl.col('elapsed_time').shift(1)).over(['session']).alias('elapsed_time_diff1'),\n",
    "        (pl.col('elapsed_time') - pl.col('elapsed_time').shift(2)).over(['session']).alias('elapsed_time_diff2'),\n",
    "        (pl.col('elapsed_time').shift(-1) - pl.col('elapsed_time')).over(['session']).alias('elapsed_time_diff3'),\n",
    "        ((pl.col('room_coor_x') - pl.col('room_coor_x').shift(1))\n",
    "         .over(['session']) ** 2 \n",
    "         + (pl.col('room_coor_y') - pl.col('room_coor_y').shift(1))\n",
    "         .over(['session']) ** 2).sqrt().alias('room_dist'),\n",
    "\n",
    "        ((pl.col('screen_coor_x') - pl.col('screen_coor_x').shift(1))\n",
    "         .over(['session']) ** 2\n",
    "         + (pl.col('screen_coor_y') - pl.col('screen_coor_y').shift(1))\n",
    "         .over(['session']) ** 2).sqrt().alias('screen_dist'),\n",
    "        (pl.col('index')/(pl.col('index').max().over(['session'])-pl.col('index').min().over(['session']))).alias('index_ratio'),\n",
    "        (pl.col('elapsed_time').max().over(['session'])).alias('end_time')\n",
    "    ]\n",
    "    tmp_logs = logs.filter(pl.col('level')<=level_limit)\n",
    "    tmp_logs = tmp_logs.with_columns(columns)\n",
    "    \n",
    "    gp = tmp_logs.groupby(['session'], maintain_order=True).agg([pl.col('elapsed_time').first().suffix('_min'),\n",
    "                                                                 pl.col('elapsed_time').last().suffix('_max'),\n",
    "                                                                 pl.col('elapsed_time_diff1').count().alias('event_cnt'),\n",
    "                                                                 pl.col('elapsed_time_diff1').n_unique().suffix('_nuniq'),\n",
    "                                                                 pl.col('elapsed_time_diff1').mean().suffix('_mean'),\n",
    "                                                                 pl.col('elapsed_time_diff1').max().suffix('_max'),\n",
    "                                                                 pl.col('elapsed_time_diff1').min().suffix('_min'),\n",
    "                                                                 pl.col('elapsed_time_diff1').std().suffix('_std'),\n",
    "                                                                 pl.col('elapsed_time_diff1').median().suffix('_med'),\n",
    "                                                                 pl.col('elapsed_time_diff1').sum().suffix('_sum'),\n",
    "                                                                 pl.col('elapsed_time_diff1').first().suffix('_first'),\n",
    "                                                                 pl.col('elapsed_time_diff1').last().suffix('_last'),\n",
    "                                                                 pl.col('elapsed_time_diff1').quantile(0.1, 'nearest').suffix('_q1'),\n",
    "                                                                 pl.col('elapsed_time_diff1').quantile(0.2, 'nearest').suffix('_q2'),\n",
    "                                                                 pl.col('elapsed_time_diff1').quantile(0.3, 'nearest').suffix('_q3'),\n",
    "                                                                 pl.col('elapsed_time_diff1').quantile(0.4, 'nearest').suffix('_q4'),\n",
    "                                                                 pl.col('elapsed_time_diff1').quantile(0.5, 'nearest').suffix('_q5'),\n",
    "                                                                 pl.col('elapsed_time_diff1').quantile(0.6, 'nearest').suffix('_q6'),\n",
    "                                                                 pl.col('elapsed_time_diff1').quantile(0.7, 'nearest').suffix('_q7'),\n",
    "                                                                 pl.col('elapsed_time_diff1').quantile(0.8, 'nearest').suffix('_q8'),\n",
    "                                                                 pl.col('elapsed_time_diff1').quantile(0.9, 'nearest').suffix('_q9'),\n",
    "                                                                ])\n",
    "    gp = gp.with_columns((pl.col('elapsed_time_max')-pl.col('elapsed_time_min')).alias('level_group_timedelta'))\n",
    "    gp.drop(['elapsed_time_max', 'elapsed_time_min'])\n",
    "    df = df.join(gp, on=['session'], how='left')\n",
    "    \n",
    "    nuniq_cols = ['event_name', 'name', 'level', 'page', 'room_coor_x',\n",
    "                  'room_coor_y', 'screen_coor_x', 'screen_coor_y', 'hover_duration', 'text', \n",
    "                  'fqid', 'room_fqid', 'text_fqid']\n",
    "    agg_list = [pl.col(col).n_unique().suffix('_nuniq') for col in nuniq_cols]\n",
    "    gp = tmp_logs.groupby(['session'], maintain_order=True).agg([*agg_list])\n",
    "    df = df.join(gp, on=['session'], how='left')\n",
    "    \n",
    "    for key in ['level', 'room_fqid', 'fqid', 'text_fqid', 'event_name']:\n",
    "        gp = tmp_logs.groupby(['session', key], maintain_order=True).agg([pl.col('elapsed_time').first().alias('elapsed_time_min'),\n",
    "                                                     pl.col('elapsed_time').last().alias('elapsed_time_max')])\n",
    "        gp = gp.with_columns((pl.col('elapsed_time_max')-pl.col('elapsed_time_min')).alias(f'{key}_timedelta'))\n",
    "        gp.drop(['elapsed_time_max', 'elapsed_time_min'])\n",
    "        gp = gp.pivot(index=['session'], columns=key, values=f'{key}_timedelta', aggregate_function='sum')\n",
    "        if key=='text':\n",
    "            gp.columns = ['session'] + [f'{key}={i}_timedelta' for i,f in enumerate(gp.columns[1:])]\n",
    "        else:\n",
    "            gp.columns = ['session'] + [f'{key}={f}_timedelta' for f in gp.columns[1:]]\n",
    "        df = df.join(gp, on=['session'], how='left')\n",
    "        \n",
    "    for key in ['level', 'room_fqid', 'fqid', 'text_fqid', 'event_name',\n",
    "                'text'\n",
    "               ]:\n",
    "        gp = tmp_logs.groupby(['session', key], maintain_order=True).agg([pl.col('elapsed_time_diff1').mean()])\n",
    "        gp = gp.pivot(index=['session'], columns=key, values=f'elapsed_time_diff1', aggregate_function='sum')\n",
    "        if key=='text':\n",
    "            gp.columns = ['session'] + [f'{key}={texts_map.get(f, f)}_elapsed_time_diff1' for i,f in enumerate(gp.columns[1:])]\n",
    "        else:\n",
    "            gp.columns = ['session'] + [f'{key}={f}_elapsed_time_diff1' for f in gp.columns[1:]]\n",
    "        df = df.join(gp, on=['session'], how='left')\n",
    "        \n",
    "    for key in ['level', 'room_fqid', 'fqid', 'text_fqid', 'event_name',\n",
    "                'text'\n",
    "               ]:\n",
    "        gp = tmp_logs.groupby(['session', key], maintain_order=True).agg([pl.col('elapsed_time_diff1').count()])\n",
    "        gp = gp.pivot(index=['session'], columns=key, values=f'elapsed_time_diff1', aggregate_function='sum')\n",
    "        if key=='text':\n",
    "            gp.columns = ['session'] + [f'{key}={texts_map.get(f, f)}_cnt' for i,f in enumerate(gp.columns[1:])]\n",
    "        else:\n",
    "            gp.columns = ['session'] + [f'{key}={f}_cnt' for f in gp.columns[1:]]\n",
    "        df = df.join(gp, on=['session'], how='left')\n",
    "        \n",
    "    for key in ['level', 'room_fqid', 'fqid', 'text_fqid', 'event_name']:\n",
    "        gp = tmp_logs.groupby(['session', key], maintain_order=True).agg([pl.col('index').min().alias('elapsed_time_min'),\n",
    "                                                     pl.col('index').max().alias('elapsed_time_max')])\n",
    "        gp = gp.with_columns((pl.col('elapsed_time_max')-pl.col('elapsed_time_min')).alias(f'{key}_index_gap'))\n",
    "        gp.drop(['elapsed_time_max', 'elapsed_time_min'])\n",
    "        gp = gp.pivot(index=['session'], columns=key, values=f'{key}_index_gap', aggregate_function='sum')\n",
    "        if key=='text':\n",
    "            gp.columns = ['session'] + [f'{key}={i}_index_gap' for i,f in enumerate(gp.columns[1:])]\n",
    "        else:\n",
    "            gp.columns = ['session'] + [f'{key}={f}_index_gap' for f in gp.columns[1:]]\n",
    "        df = df.join(gp, on=['session'], how='left')\n",
    "               \n",
    "    for key in ['room_fqid', 'fqid', 'text_fqid']:\n",
    "        gp = tmp_logs.groupby(['session', key], maintain_order=True).agg([pl.col('event_name').n_unique()])\n",
    "        gp = gp.pivot(index=['session'], columns=key, values=f'event_name', aggregate_function='sum')\n",
    "        if key=='text':\n",
    "            gp.columns = ['session'] + [f'{key}={i}_event_name_nuniq' for i,f in enumerate(gp.columns[1:])]\n",
    "        else:\n",
    "            gp.columns = ['session'] + [f'{key}={f}_event_name_nuniq' for f in gp.columns[1:]]\n",
    "        df = df.join(gp, on=['session'], how='left')\n",
    "        \n",
    "    if level_limit>=5 and level_limit<=12:\n",
    "        for key in ['room_fqid', 'fqid', 'text_fqid']:\n",
    "            gp = tmp_logs.groupby(['session', key], maintain_order=True).agg([pl.col('event_name').count()])\n",
    "            gp = gp.groupby(['session'], maintain_order=True).agg([\n",
    "                 pl.col('event_name').mean().suffix(f'_{key}_cnt_mean'),\n",
    "                                                                     pl.col('event_name').max().suffix(f'_{key}_cnt_max'),\n",
    "                                                                     pl.col('event_name').min().suffix(f'_{key}_cnt_min'),\n",
    "                                                                     pl.col('event_name').std().suffix(f'_{key}_cnt_std'),\n",
    "                                                                     pl.col('event_name').median().suffix(f'_{key}_cnt_med'),\n",
    "                                                                     pl.col('event_name').sum().suffix(f'_{key}_cnt_sum'),\n",
    "                                                                     pl.col('event_name').first().suffix(f'_{key}_cnt_first'),\n",
    "                                                                     pl.col('event_name').last().suffix(f'_{key}_cnt_last'),\n",
    "\n",
    "            ])\n",
    "            df = df.join(gp, on=['session'], how='left')\n",
    "        \n",
    "    for window in [5, 10, 15, 30]:\n",
    "        nuniq_cols = ['event_name', 'name', 'level', 'page', 'room_coor_x',\n",
    "                      'room_coor_y', 'screen_coor_x', 'screen_coor_y', 'hover_duration', 'text', \n",
    "                      'fqid', 'room_fqid', 'text_fqid']\n",
    "        agg_list = [pl.col(f).n_unique().suffix(f'_nuniq_{window}sec') for f in nuniq_cols]\n",
    "        tmp = tmp_logs.filter(pl.col('elapsed_time')>pl.col('end_time')-window*1000)\n",
    "        gp = tmp.groupby(['session'], maintain_order=True).agg([*agg_list])\n",
    "        df = df.join(gp, on=['session'], how='left')\n",
    "        \n",
    "    for col in ['event_name', 'name', 'page', 'room_coor_x',\n",
    "                'room_coor_y', 'screen_coor_x', 'screen_coor_y', 'hover_duration', 'text', \n",
    "                'fqid', 'text_fqid']:\n",
    "        gp = tmp_logs.groupby(['session', 'room_fqid'], maintain_order=True).agg([pl.col(col).n_unique()])\n",
    "        gp = gp.pivot(index=['session'], columns='room_fqid', values=col, aggregate_function='sum')\n",
    "        gp.columns = ['session'] + [f'room_fqid={f}_{col}_nuniq' for f in gp.columns[1:]]\n",
    "        df = df.join(gp, on=['session'], how='left')\n",
    "        \n",
    "    for key in [\n",
    "        'event_name',\n",
    "               ]:\n",
    "        gp = tmp_logs.groupby(['session', key], maintain_order=True).agg([pl.col('elapsed_time_diff1').last()])\n",
    "        gp = gp.pivot(index=['session'], columns=key, values=f'elapsed_time_diff1', aggregate_function='sum')\n",
    "        if key=='text':\n",
    "            gp.columns = ['session'] + [f'{key}={i}_elapsed_time_diff1_last' for i,f in enumerate(gp.columns[1:])]\n",
    "        else:\n",
    "            gp.columns = ['session'] + [f'{key}={f}_elapsed_time_diff1_last' for f in gp.columns[1:]]\n",
    "        df = df.join(gp, on=['session'], how='left')\n",
    "        \n",
    "    for key in ['level', 'room_fqid', 'fqid', 'text_fqid', 'event_name',\n",
    "                'text'\n",
    "               ]:\n",
    "        gp = tmp_logs.groupby(['session', key], maintain_order=True).agg([pl.col('elapsed_time_diff1').std()])\n",
    "        gp = gp.pivot(index=['session'], columns=key, values=f'elapsed_time_diff1', aggregate_function='sum')\n",
    "        if key=='text':\n",
    "            gp.columns = ['session'] + [f'{key}={texts_map.get(f, f)}_elapsed_time_diff1_std' for i,f in enumerate(gp.columns[1:])]\n",
    "        else:\n",
    "            gp.columns = ['session'] + [f'{key}={f}_elapsed_time_diff1_std' for f in gp.columns[1:]]\n",
    "        df = df.join(gp, on=['session'], how='left')\n",
    "        \n",
    "        \n",
    "    for key in ['level', 'room_fqid', 'fqid', 'text_fqid', 'event_name',\n",
    "                'text'\n",
    "               ]:\n",
    "        gp = tmp_logs.groupby(['session', key], maintain_order=True).agg([pl.col('elapsed_time_diff1').max()])\n",
    "        gp = gp.pivot(index=['session'], columns=key, values=f'elapsed_time_diff1', aggregate_function='sum')\n",
    "        if key=='text':\n",
    "            gp.columns = ['session'] + [f'{key}={texts_map.get(f, f)}_elapsed_time_diff1_max' for i,f in enumerate(gp.columns[1:])]\n",
    "        else:\n",
    "            gp.columns = ['session'] + [f'{key}={f}_elapsed_time_diff1_max' for f in gp.columns[1:]]\n",
    "        df = df.join(gp, on=['session'], how='left')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd1260d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-26T06:56:22.754633Z",
     "iopub.status.busy": "2023-06-26T06:56:22.753996Z",
     "iopub.status.idle": "2023-06-26T06:56:22.821758Z",
     "shell.execute_reply": "2023-06-26T06:56:22.820810Z"
    },
    "papermill": {
     "duration": 0.076248,
     "end_time": "2023-06-26T06:56:22.824303",
     "exception": false,
     "start_time": "2023-06-26T06:56:22.748055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "\n",
    "    pl.col(\"page\").cast(pl.Float32),\n",
    "    (\n",
    "        (pl.col(\"elapsed_time\") - pl.col(\"elapsed_time\").shift(1))\n",
    "         .fill_null(0)\n",
    "         .clip(0, 1e9)\n",
    "         .over([\"session_id\", \"level_group\"])\n",
    "         .alias(\"elapsed_time_diff\")\n",
    "    ),\n",
    "    (\n",
    "        (pl.col(\"elapsed_time\") - pl.col(\"elapsed_time\").shift(-1))\n",
    "         .abs()\n",
    "         .fill_null(0)\n",
    "         .clip(0, 1e9)\n",
    "         .over([\"session_id\", \"level_group\"])\n",
    "         .alias(\"elapsed_time_diff2\")\n",
    "    ),\n",
    "    (\n",
    "        (pl.col(\"screen_coor_x\") - pl.col(\"screen_coor_x\").shift(1)) \n",
    "         .abs()\n",
    "         .over([\"session_id\", \"level_group\"])\n",
    "        .alias(\"location_x_diff\") \n",
    "    ),\n",
    "    (\n",
    "        (pl.col(\"screen_coor_x\") - pl.col(\"screen_coor_x\").shift(-1)) \n",
    "         .abs()\n",
    "         .over([\"session_id\", \"level_group\"])\n",
    "        .alias(\"location_x_diff2\") \n",
    "    ),\n",
    "    (\n",
    "        (pl.col(\"screen_coor_y\") - pl.col(\"screen_coor_y\").shift(1)) \n",
    "         .abs()\n",
    "         .over([\"session_id\", \"level_group\"])\n",
    "        .alias(\"location_y_diff\") \n",
    "    ),\n",
    "]\n",
    "\n",
    "CATS = ['event_name', 'name', 'fqid', 'room_fqid', 'text_fqid','text']\n",
    "NUMS = ['page', 'room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y',\n",
    "        'hover_duration', 'elapsed_time_diff']\n",
    "\n",
    "name_feature = ['basic', 'undefined', 'close', 'open', 'prev', 'next']\n",
    "event_name_feature = ['cutscene_click', 'person_click', 'navigate_click',\n",
    "       'observation_click', 'notification_click', 'object_click',\n",
    "       'object_hover', 'map_hover', 'map_click', 'checkpoint',\n",
    "       'notebook_click']\n",
    "\n",
    "fqid_lists = ['worker', 'archivist', 'gramps', 'wells', 'toentry', 'confrontation', 'crane_ranger', 'groupconvo', 'flag_girl', 'tomap', 'tostacks', 'tobasement', 'archivist_glasses', 'boss', 'journals', 'seescratches', 'groupconvo_flag', 'cs', 'teddy', 'expert', 'businesscards', 'ch3start', 'tunic.historicalsociety', 'tofrontdesk', 'savedteddy', 'plaque', 'glasses', 'tunic.drycleaner', 'reader_flag', 'tunic.library', 'tracks', 'tunic.capitol_2', 'trigger_scarf', 'reader', 'directory', 'tunic.capitol_1', 'journals.pic_0.next', 'unlockdoor', 'tunic', 'what_happened', 'tunic.kohlcenter', 'tunic.humanecology', 'colorbook', 'logbook', 'businesscards.card_0.next', 'journals.hub.topics', 'logbook.page.bingo', 'journals.pic_1.next', 'journals_flag', 'reader.paper0.next', 'tracks.hub.deer', 'reader_flag.paper0.next', 'trigger_coffee', 'wellsbadge', 'journals.pic_2.next', 'tomicrofiche', 'journals_flag.pic_0.bingo', 'plaque.face.date', 'notebook', 'tocloset_dirty', 'businesscards.card_bingo.bingo', 'businesscards.card_1.next', 'tunic.wildlife', 'tunic.hub.slip', 'tocage', 'journals.pic_2.bingo', 'tocollectionflag', 'tocollection', 'chap4_finale_c', 'chap2_finale_c', 'lockeddoor', 'journals_flag.hub.topics', 'tunic.capitol_0', 'reader_flag.paper2.bingo', 'photo', 'tunic.flaghouse', 'reader.paper1.next', 'directory.closeup.archivist', 'intro', 'businesscards.card_bingo.next', 'reader.paper2.bingo', 'retirement_letter', 'remove_cup', 'journals_flag.pic_0.next', 'magnify', 'coffee', 'key', 'togrampa', 'reader_flag.paper1.next', 'janitor', 'tohallway', 'chap1_finale', 'report', 'outtolunch', 'journals_flag.hub.topics_old', 'journals_flag.pic_1.next', 'reader.paper2.next', 'chap1_finale_c', 'reader_flag.paper2.next', 'door_block_talk', 'journals_flag.pic_1.bingo', 'journals_flag.pic_2.next', 'journals_flag.pic_2.bingo', 'block_magnify', 'reader.paper0.prev', 'block', 'reader_flag.paper0.prev', 'block_0', 'door_block_clean', 'reader.paper2.prev', 'reader.paper1.prev', 'doorblock', 'tocloset', 'reader_flag.paper2.prev', 'reader_flag.paper1.prev', 'block_tomap2', 'journals_flag.pic_0_old.next', 'journals_flag.pic_1_old.next', 'block_tocollection', 'block_nelson', 'journals_flag.pic_2_old.next', 'block_tomap1', 'block_badge', 'need_glasses', 'block_badge_2', 'fox', 'block_1']\n",
    "text_lists = ['tunic.historicalsociety.cage.confrontation', 'tunic.wildlife.center.crane_ranger.crane', 'tunic.historicalsociety.frontdesk.archivist.newspaper', 'tunic.historicalsociety.entry.groupconvo', 'tunic.wildlife.center.wells.nodeer', 'tunic.historicalsociety.frontdesk.archivist.have_glass', 'tunic.drycleaner.frontdesk.worker.hub', 'tunic.historicalsociety.closet_dirty.gramps.news', 'tunic.humanecology.frontdesk.worker.intro', 'tunic.historicalsociety.frontdesk.archivist_glasses.confrontation', 'tunic.historicalsociety.basement.seescratches', 'tunic.historicalsociety.collection.cs', 'tunic.flaghouse.entry.flag_girl.hello', 'tunic.historicalsociety.collection.gramps.found', 'tunic.historicalsociety.basement.ch3start', 'tunic.historicalsociety.entry.groupconvo_flag', 'tunic.library.frontdesk.worker.hello', 'tunic.library.frontdesk.worker.wells', 'tunic.historicalsociety.collection_flag.gramps.flag', 'tunic.historicalsociety.basement.savedteddy', 'tunic.library.frontdesk.worker.nelson', 'tunic.wildlife.center.expert.removed_cup', 'tunic.library.frontdesk.worker.flag', 'tunic.historicalsociety.frontdesk.archivist.hello', 'tunic.historicalsociety.closet.gramps.intro_0_cs_0', 'tunic.historicalsociety.entry.boss.flag', 'tunic.flaghouse.entry.flag_girl.symbol', 'tunic.historicalsociety.closet_dirty.trigger_scarf', 'tunic.drycleaner.frontdesk.worker.done', 'tunic.historicalsociety.closet_dirty.what_happened', 'tunic.wildlife.center.wells.animals', 'tunic.historicalsociety.closet.teddy.intro_0_cs_0', 'tunic.historicalsociety.cage.glasses.afterteddy', 'tunic.historicalsociety.cage.teddy.trapped', 'tunic.historicalsociety.cage.unlockdoor', 'tunic.historicalsociety.stacks.journals.pic_2.bingo', 'tunic.historicalsociety.entry.wells.flag', 'tunic.humanecology.frontdesk.worker.badger', 'tunic.historicalsociety.stacks.journals_flag.pic_0.bingo', 'tunic.historicalsociety.closet.intro', 'tunic.historicalsociety.closet.retirement_letter.hub', 'tunic.historicalsociety.entry.directory.closeup.archivist', 'tunic.historicalsociety.collection.tunic.slip', 'tunic.kohlcenter.halloffame.plaque.face.date', 'tunic.historicalsociety.closet_dirty.trigger_coffee', 'tunic.drycleaner.frontdesk.logbook.page.bingo', 'tunic.library.microfiche.reader.paper2.bingo', 'tunic.kohlcenter.halloffame.togrampa', 'tunic.capitol_2.hall.boss.haveyougotit', 'tunic.wildlife.center.wells.nodeer_recap', 'tunic.historicalsociety.cage.glasses.beforeteddy', 'tunic.historicalsociety.closet_dirty.gramps.helpclean', 'tunic.wildlife.center.expert.recap', 'tunic.historicalsociety.frontdesk.archivist.have_glass_recap', 'tunic.historicalsociety.stacks.journals_flag.pic_1.bingo', 'tunic.historicalsociety.cage.lockeddoor', 'tunic.historicalsociety.stacks.journals_flag.pic_2.bingo', 'tunic.historicalsociety.collection.gramps.lost', 'tunic.historicalsociety.closet.notebook', 'tunic.historicalsociety.frontdesk.magnify', 'tunic.humanecology.frontdesk.businesscards.card_bingo.bingo', 'tunic.wildlife.center.remove_cup', 'tunic.library.frontdesk.wellsbadge.hub', 'tunic.wildlife.center.tracks.hub.deer', 'tunic.historicalsociety.frontdesk.key', 'tunic.library.microfiche.reader_flag.paper2.bingo', 'tunic.flaghouse.entry.colorbook', 'tunic.wildlife.center.coffee', 'tunic.capitol_1.hall.boss.haveyougotit', 'tunic.historicalsociety.basement.janitor', 'tunic.historicalsociety.collection_flag.gramps.recap', 'tunic.wildlife.center.wells.animals2', 'tunic.flaghouse.entry.flag_girl.symbol_recap', 'tunic.historicalsociety.closet_dirty.photo', 'tunic.historicalsociety.stacks.outtolunch', 'tunic.library.frontdesk.worker.wells_recap', 'tunic.historicalsociety.frontdesk.archivist_glasses.confrontation_recap', 'tunic.capitol_0.hall.boss.talktogramps', 'tunic.historicalsociety.closet.photo', 'tunic.historicalsociety.collection.tunic', 'tunic.historicalsociety.closet.teddy.intro_0_cs_5', 'tunic.historicalsociety.closet_dirty.gramps.archivist', 'tunic.historicalsociety.closet_dirty.door_block_talk', 'tunic.historicalsociety.entry.boss.flag_recap', 'tunic.historicalsociety.frontdesk.archivist.need_glass_0', 'tunic.historicalsociety.entry.wells.talktogramps', 'tunic.historicalsociety.frontdesk.block_magnify', 'tunic.historicalsociety.frontdesk.archivist.foundtheodora', 'tunic.historicalsociety.closet_dirty.gramps.nothing', 'tunic.historicalsociety.closet_dirty.door_block_clean', 'tunic.capitol_1.hall.boss.writeitup', 'tunic.library.frontdesk.worker.nelson_recap', 'tunic.library.frontdesk.worker.hello_short', 'tunic.historicalsociety.stacks.block', 'tunic.historicalsociety.frontdesk.archivist.need_glass_1', 'tunic.historicalsociety.entry.boss.talktogramps', 'tunic.historicalsociety.frontdesk.archivist.newspaper_recap', 'tunic.historicalsociety.entry.wells.flag_recap', 'tunic.drycleaner.frontdesk.worker.done2', 'tunic.library.frontdesk.worker.flag_recap', 'tunic.humanecology.frontdesk.block_0', 'tunic.library.frontdesk.worker.preflag', 'tunic.historicalsociety.basement.gramps.seeyalater', 'tunic.flaghouse.entry.flag_girl.hello_recap', 'tunic.historicalsociety.closet.doorblock', 'tunic.drycleaner.frontdesk.worker.takealook', 'tunic.historicalsociety.basement.gramps.whatdo', 'tunic.library.frontdesk.worker.droppedbadge', 'tunic.historicalsociety.entry.block_tomap2', 'tunic.library.frontdesk.block_nelson', 'tunic.library.microfiche.block_0', 'tunic.historicalsociety.entry.block_tocollection', 'tunic.historicalsociety.entry.block_tomap1', 'tunic.historicalsociety.collection.gramps.look_0', 'tunic.library.frontdesk.block_badge', 'tunic.historicalsociety.cage.need_glasses', 'tunic.library.frontdesk.block_badge_2', 'tunic.kohlcenter.halloffame.block_0', 'tunic.capitol_0.hall.chap1_finale_c', 'tunic.capitol_1.hall.chap2_finale_c', 'tunic.capitol_2.hall.chap4_finale_c', 'tunic.wildlife.center.fox.concern', 'tunic.drycleaner.frontdesk.block_0', 'tunic.historicalsociety.entry.gramps.hub', 'tunic.humanecology.frontdesk.block_1', 'tunic.drycleaner.frontdesk.block_1']\n",
    "room_lists = ['tunic.historicalsociety.entry', 'tunic.wildlife.center', 'tunic.historicalsociety.cage', 'tunic.library.frontdesk', 'tunic.historicalsociety.frontdesk', 'tunic.historicalsociety.stacks', 'tunic.historicalsociety.closet_dirty', 'tunic.humanecology.frontdesk', 'tunic.historicalsociety.basement', 'tunic.kohlcenter.halloffame', 'tunic.library.microfiche', 'tunic.drycleaner.frontdesk', 'tunic.historicalsociety.collection', 'tunic.historicalsociety.closet', 'tunic.flaghouse.entry', 'tunic.historicalsociety.collection_flag', 'tunic.capitol_1.hall', 'tunic.capitol_0.hall', 'tunic.capitol_2.hall']\n",
    "\n",
    "def feature_engineer_pl(x, grp, use_extra, feature_suffix):\n",
    "    # The function takes four arguments: x (DataFrame), grp (grouping column), use_extra (boolean flag), and feature_suffix (suffix to be appended to the feature names).\n",
    "    # The aggs list is initialized to store the aggregation expressions for feature engineering.\n",
    "    # Multiple aggregation expressions are added to the aggs list. These expressions calculate various statistics and counts based on different columns in the DataFrame. Some examples include computing quantiles, means, minimum and maximum values, counts, and sums for different columns.\n",
    "    # The DataFrame x is grouped by the \"session_id\" column and the aggregation expressions from aggs are applied. The resulting DataFrame is sorted by \"session_id\" and assigned to df.\n",
    "    # If the use_extra flag is True, additional feature engineering is performed.\n",
    "    # If the value \"5-12\" is present in the \"level_group\" column of x, a new aggregation expression is added to calculate the time difference between the first \"5-12\" level group and the last \"0-4\" level group. The result is stored in the \"time_group_diff1\" column.\n",
    "    # If the value \"13-22\" is present in the \"level_group\" column of x, a new aggregation expression is added to calculate the time difference between the first \"13-22\" level group and the last \"5-12\" level group. The result is stored in the \"time_group_diff2\" column.\n",
    "    # The resulting DataFrame df is joined with the additional features (tmp) based on the \"session_id\" column.\n",
    "    # Finally, the function returns the feature-engineered DataFrame df.\n",
    "    aggs = [\n",
    "        *[pl.col(c).quantile(0.1, \"nearest\").alias(f\"{c}_quantile1_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.2, \"nearest\").alias(f\"{c}_quantile2_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.4, \"nearest\").alias(f\"{c}_quantile4_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.6, \"nearest\").alias(f\"{c}_quantile6_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.8, \"nearest\").alias(f\"{c}_quantile8_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.9, \"nearest\").alias(f\"{c}_quantile9_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).mean().alias(f\"{c}_mean_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).min().alias(f\"{c}_min_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).max().alias(f\"{c}_max_{feature_suffix}\") for c in NUMS],\n",
    "        \n",
    "        *[pl.col(\"event_name\").filter(pl.col(\"event_name\") == c).count().alias(f\"{c}_event_name_counts{feature_suffix}\")for c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\")==c).quantile(0.1, \"nearest\").alias(f\"{c}_ET_quantile1_{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\")==c).quantile(0.2, \"nearest\").alias(f\"{c}_ET_quantile2_{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\")==c).quantile(0.4, \"nearest\").alias(f\"{c}_ET_quantile4_{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\")==c).quantile(0.6, \"nearest\").alias(f\"{c}_ET_quantile6_{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\")==c).quantile(0.8, \"nearest\").alias(f\"{c}_ET_quantile8_{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\")==c).quantile(0.9, \"nearest\").alias(f\"{c}_ET_quantile9_{feature_suffix}\") for c in event_name_feature],      \n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\")==c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\")==c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\")==c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\")==c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff2\").filter(pl.col(\"event_name\")==c).sum().alias(f\"{c}_ET2_sum_{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"location_x_diff\").filter(pl.col(\"event_name\")==c).mean().alias(f\"{c}_LX_mean_x{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"location_x_diff\").filter(pl.col(\"event_name\")==c).max().alias(f\"{c}_LX_max_x{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"location_x_diff\").filter(pl.col(\"event_name\")==c).min().alias(f\"{c}_LX_min_x{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"location_x_diff\").filter(pl.col(\"event_name\")==c).sum().alias(f\"{c}_LX_sum_x{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"location_x_diff2\").filter(pl.col(\"event_name\")==c).sum().alias(f\"{c}_LX2_sum_x{feature_suffix}\") for c in event_name_feature],\n",
    "        *[pl.col(\"location_y_diff\").filter(pl.col(\"event_name\")==c).sum().alias(f\"{c}_LY_sum_x{feature_suffix}\") for c in event_name_feature],\n",
    "     \n",
    "        *[pl.col(\"name\").filter(pl.col(\"name\") == c).count().alias(f\"{c}_name_counts{feature_suffix}\")for c in name_feature],   \n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\")==c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\")==c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\")==c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\")==c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff2\").filter(pl.col(\"name\")==c).sum().alias(f\"{c}_ET2_sum_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"location_x_diff\").filter(pl.col(\"name\")==c).sum().alias(f\"{c}_LX_sum_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"location_x_diff2\").filter(pl.col(\"name\")==c).sum().alias(f\"{c}_LX2_sum_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"location_y_diff\").filter(pl.col(\"name\")==c).sum().alias(f\"{c}_LY_sum_{feature_suffix}\") for c in name_feature],\n",
    "        \n",
    "        *[pl.col(\"room_fqid\").filter(pl.col(\"room_fqid\") == c).count().alias(f\"{c}_room_fqid_counts{feature_suffix}\")for c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in room_lists],\n",
    "        *[pl.col(\"elapsed_time_diff2\").filter(pl.col(\"room_fqid\") == c).sum().alias(f\"{c}_ET2_sum_{feature_suffix}\") for c in room_lists],\n",
    "        *[pl.col(\"location_x_diff\").filter(pl.col(\"room_fqid\") == c).sum().alias(f\"{c}_LX_sum_{feature_suffix}\") for c in room_lists],\n",
    "        *[pl.col(\"location_x_diff2\").filter(pl.col(\"room_fqid\") == c).sum().alias(f\"{c}_LX2_sum_{feature_suffix}\") for c in room_lists],\n",
    "        *[pl.col(\"location_y_diff\").filter(pl.col(\"room_fqid\") == c).sum().alias(f\"{c}_LY_sum_{feature_suffix}\") for c in room_lists],\n",
    "                \n",
    "        *[pl.col(\"fqid\").filter(pl.col(\"fqid\") == c).count().alias(f\"{c}_fqid_counts{feature_suffix}\")for c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in fqid_lists],\n",
    "        *[pl.col(\"elapsed_time_diff2\").filter(pl.col(\"fqid\") == c).sum().alias(f\"{c}_ET2_sum_{feature_suffix}\") for c in fqid_lists],\n",
    "        *[pl.col(\"location_x_diff\").filter(pl.col(\"fqid\") == c).sum().alias(f\"{c}_LX_sum_{feature_suffix}\") for c in fqid_lists],\n",
    "        *[pl.col(\"location_x_diff2\").filter(pl.col(\"fqid\") == c).sum().alias(f\"{c}_LX2_sum_{feature_suffix}\") for c in fqid_lists],\n",
    "        *[pl.col(\"location_y_diff\").filter(pl.col(\"fqid\") == c).sum().alias(f\"{c}_LY_sum_{feature_suffix}\") for c in fqid_lists],\n",
    "       \n",
    "        *[pl.col(\"text_fqid\").filter(pl.col(\"text_fqid\") == c).count().alias(f\"{c}_text_fqid_counts{feature_suffix}\") for c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in text_lists],\n",
    "        *[pl.col(\"elapsed_time_diff2\").filter(pl.col(\"text_fqid\") == c).sum().alias(f\"{c}_ET2_sum_{feature_suffix}\") for c in text_lists],\n",
    "        *[pl.col(\"location_x_diff\").filter(pl.col(\"text_fqid\") == c).sum().alias(f\"{c}_LX_sum_{feature_suffix}\") for c in text_lists],\n",
    "        *[pl.col(\"location_x_diff2\").filter(pl.col(\"text_fqid\") == c).sum().alias(f\"{c}_LX2_sum_{feature_suffix}\") for c in text_lists],\n",
    "        *[pl.col(\"location_y_diff\").filter(pl.col(\"text_fqid\") == c).sum().alias(f\"{c}_LY_sum_{feature_suffix}\") for c in text_lists],       \n",
    "    ]\n",
    "    \n",
    "    df = x.groupby([\"session_id\"], maintain_order=True).agg(aggs).sort(\"session_id\")\n",
    "    \n",
    "    if use_extra:\n",
    "        if '5-12' in x[\"level_group\"].unique():\n",
    "            aggs = [\n",
    "                (pl.col(\"elapsed_time\").filter((pl.col(\"level_group\")==\"5-12\")).first()-pl.col(\"elapsed_time\").filter((pl.col(\"level_group\")==\"0-4\")).last()).alias(\"time_group_diff1\"),\n",
    "            ]\n",
    "            tmp = x.groupby([\"session_id\"], maintain_order=True).agg(aggs).sort(\"session_id\")\n",
    "            df = df.join(tmp, on=\"session_id\", how='left')\n",
    "\n",
    "        if '13-22' in x[\"level_group\"].unique():\n",
    "            aggs = [\n",
    "                (pl.col(\"elapsed_time\").filter((pl.col(\"level_group\")==\"13-22\")).first()-pl.col(\"elapsed_time\").filter((pl.col(\"level_group\")==\"5-12\")).last()).alias(\"time_group_diff2\"), \n",
    "            \n",
    "            ]\n",
    "            tmp = x.groupby([\"session_id\"], maintain_order=True).agg(aggs).sort(\"session_id\")\n",
    "            df = df.join(tmp, on=\"session_id\", how='left')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "458c6fb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-26T06:56:22.835067Z",
     "iopub.status.busy": "2023-06-26T06:56:22.834680Z",
     "iopub.status.idle": "2023-06-26T06:56:22.844693Z",
     "shell.execute_reply": "2023-06-26T06:56:22.843922Z"
    },
    "papermill": {
     "duration": 0.0187,
     "end_time": "2023-06-26T06:56:22.847189",
     "exception": false,
     "start_time": "2023-06-26T06:56:22.828489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models.py\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math \n",
    "# This code defines a Transformer block and a model that combines Transformer and LSTM blocks for sequence processing. The model takes both dense and categorical features as input and produces an output based on different modes. The Transformer block is used to capture the dependencies within the sequences, and the LSTM block is used for further sequence processing.\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_s=32, head_cnt=8, dp1=0.1, dp2=0.1):\n",
    "        super().__init__()\n",
    "        # The constructor initializes several components used in a transformer block, \n",
    "        # including the multi-head self-attention mechanism and feed-forward network.\n",
    "        emb = emb_s * head_cnt\n",
    "        self.kqv = nn.Linear(emb_s, 3 * emb_s, bias=False)  # Linear transformation for keys, queries, and values.\n",
    "        self.dp = nn.Dropout(dp1)  # Dropout layer.\n",
    "        self.proj = nn.Linear(emb, emb, bias=False)  # Projection layer for the output of the self-attention mechanism.\n",
    "        self.head_cnt = head_cnt  # The number of attention heads.\n",
    "        self.emb_s = emb_s  # The dimension of embeddings.\n",
    "        self.ln1 = nn.LayerNorm(emb)  # Layer normalization.\n",
    "        self.ln2 = nn.LayerNorm(emb)  # Layer normalization.\n",
    "        self.alpha1 = nn.Parameter(torch.ones(1) * 0.1, requires_grad=True)  # Scaling factor for the self-attention mechanism.\n",
    "        self.alpha2 = nn.Parameter(torch.ones(1) * 0.1, requires_grad=True)  # Scaling factor for the feed-forward network.\n",
    "\n",
    "        # The feed-forward network, consisting of two linear layers and a ReLU activation function.\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb, 4 * emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * emb, emb),\n",
    "            nn.Dropout(dp2),\n",
    "        )\n",
    "\n",
    "    def resmha(self, x, prev=None, mask=None):\n",
    "        # This function implements the residual multi-head attention mechanism.\n",
    "        B, T, _ = x.shape  # Batch size, sequence length, and embedding size.\n",
    "        x = x.reshape(B, T, self.head_cnt, self.emb_s)\n",
    "        k, q, v = torch.split(self.kqv(x), self.emb_s, dim=-1)  # Split the input into keys, queries, and values.\n",
    "\n",
    "        # Calculate the attention scores. If a previous attention score matrix is given, add it to the current scores.\n",
    "        if prev is not None:\n",
    "            att_score = torch.einsum('bihk,bjhk->bijh', q, k) / self.emb_s ** 0.5 + prev\n",
    "        else:\n",
    "            att_score = torch.einsum('bihk,bjhk->bijh', q, k) / self.emb_s ** 0.5\n",
    "\n",
    "        # If a mask is provided, apply it to the attention scores.\n",
    "        if mask is not None:\n",
    "            att_score = att_score.masked_fill(mask.unsqueeze(1).unsqueeze(-1)==0, -1e9)\n",
    "        prev = att_score\n",
    "\n",
    "        # Apply the softmax function to the attention scores to obtain the attention weights, \n",
    "        # and use them to calculate the weighted sum of the values.\n",
    "        att = F.softmax(prev, dim=2)\n",
    "        res = torch.einsum('btih,bihs->bths', att, v).reshape(B, T, -1)\n",
    "\n",
    "        # Apply dropout and a linear transformation to the result.\n",
    "        return self.dp(self.proj(res)), prev\n",
    "\n",
    "    def forward(self, x, prev=None, mask=None):\n",
    "        # This function implements the forward pass of the transformerSure, let's continue from where it left off:\n",
    "        # First, it applies the residual multi-head self-attention mechanism to the input.\n",
    "        rmha, prev = self.resmha(x, prev=prev, mask=mask)\n",
    "\n",
    "        # Then, it adds the output of the self-attention mechanism (scaled by alpha1) to the input and applies layer normalization.\n",
    "        x = self.ln1(x + self.alpha1 * rmha)\n",
    "\n",
    "        # Finally, it applies the feed-forward network to the result (scaled by alpha2), adds the output to the input, \n",
    "        # and applies layer normalization again.\n",
    "        x = self.ln2(x + self.alpha2 * self.ff(x))\n",
    "\n",
    "        # The function returns the final output and the attention scores.\n",
    "        return x, prev\n",
    "\n",
    "\n",
    "class MyModelV1(nn.Module):\n",
    "    def __init__(self, model_name, in_dim1, n_categories: list, cat_dim=16, hidden_dim=256, use_cat=True):\n",
    "        super(MyModelV1, self).__init__()\n",
    "        # The constructor initializes several components used in the model, \n",
    "        # including the Transformer and LSTM blocks, and a feed-forward network for the final output.\n",
    "        \n",
    "        # The model name determines the number of layers, attention heads, and embedding size in the model.\n",
    "        if model_name == 'v1':\n",
    "            layer_cnt, head_cnt, embedding_dim = 1, 8, 64\n",
    "        # The model parameters are the same for 'v2', 'v3', and 'v4'\n",
    "        elif model_name in ['v2', 'v3', 'v4']:\n",
    "            layer_cnt, head_cnt, embedding_dim = 1, 8, 64\n",
    "        else:\n",
    "            layer_cnt, head_cnt, embedding_dim = 1, 12, 64\n",
    "        \n",
    "        # If categorical features are used, initialize the embedding layers for them.\n",
    "        if use_cat:\n",
    "            self.cats = nn.ModuleList([\n",
    "                nn.Embedding(x, cat_dim) for x in n_categories])\n",
    "            self.cat_dim = cat_dim * len(n_categories)\n",
    "\n",
    "            # The first fully connected layer takes the concatenated dense and categorical features as input.\n",
    "            self.fc = nn.Sequential( \n",
    "                nn.Linear(in_dim1 + self.cat_dim, embedding_dim * head_cnt),\n",
    "            )\n",
    "        else:\n",
    "            # If categorical features are not used, the first fully connected layer only takes the dense features as input.\n",
    "            self.fc = nn.Sequential( \n",
    "                nn.Linear(in_dim1, embedding_dim * head_cnt),\n",
    "            )\n",
    "        self.use_cat = use_cat\n",
    "        \n",
    "        # Initialize the Transformer and LSTM blocks.\n",
    "        self.transformer_encoder = nn.Sequential(\n",
    "            *[TransformerBlock(emb_s=embedding_dim, head_cnt=head_cnt, dp1=0.1, dp2=0.1) for _ in range(layer_cnt)])\n",
    "        self.lstm_encoder = nn.Sequential(\n",
    "            *[nn.LSTM(head_cnt * embedding_dim, head_cnt * embedding_dim // 2, num_layers=1, \n",
    "                              dropout=0.1, batch_first=True,\n",
    "                              bidirectional=True) for _ in range(layer_cnt)])\n",
    "\n",
    "        # Initialize the final fully connected layers for different modes.\n",
    "        self.last_fc1 = nn.Linear(embedding_dim * head_cnt , 3)\n",
    "        self.last_fc2 = nn.Linear(embedding_dim * head_cnt , 10)\n",
    "        self.last_fc3 = nn.Linear(embedding_dim * head_cnt , 5)\n",
    "        \n",
    "        # Initialize the sigmoid activation function for the final output.\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, dense_features, cat_features, mode=0):\n",
    "        # This function implements the forward pass of the model.\n",
    "        \n",
    "        # If categorical features are used, concatenate them with the dense features.\n",
    "        if self.use_cat:\n",
    "            cats = [embedding(cat_features[:, :, i]) for i, embedding in enumerate(self.cats)]\n",
    "            x_cat_emb = torch.cat(cats, 2) \n",
    "            dense_features = torch.cat([dense_features, x_cat_emb], 2)\n",
    "        \n",
    "        # Apply the first fully connected layer to the features.\n",
    "        res = self.fc(dense_features)\n",
    "\n",
    "        # Apply the Transformer and LSTM blocks to the result.\n",
    "        prev = None\n",
    "        for i in range(self.layer_cnt):\n",
    "            res, prev = self.transformer_encoder[i](res, prev)\n",
    "            res,_ = self.lstm_encoder[i](res)\n",
    "        \n",
    "        res = res.mean(1)\n",
    "                \n",
    "        if mode == 0:\n",
    "            output = self.last_fc1(res)\n",
    "        elif mode == 1:\n",
    "            output = self.last_fc2(res)\n",
    "        elif mode == 2:\n",
    "            output = self.last_fc3(res)\n",
    "        output = self.sig(output)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "612ace01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-26T06:56:22.857449Z",
     "iopub.status.busy": "2023-06-26T06:56:22.856854Z",
     "iopub.status.idle": "2023-06-26T06:56:37.763414Z",
     "shell.execute_reply": "2023-06-26T06:56:37.762281Z"
    },
    "papermill": {
     "duration": 14.914215,
     "end_time": "2023-06-26T06:56:37.765631",
     "exception": false,
     "start_time": "2023-06-26T06:56:22.851416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "parameter = Parameter()\n",
    "parameter.set(**{'batch_size': 1, 'n_jobs': 1})\n",
    "\n",
    "def random_sample(dense_features, cat_features):\n",
    "    # Randomly select indices for sampling\n",
    "    init_size = len(dense_features)\n",
    "    index = np.random.choice(range(init_size), size=parameter.seq_length, replace=False)\n",
    "    index.sort()\n",
    "    \n",
    "    # Sample the dense and categorical features based on the selected indices\n",
    "    dense_features, cat_features = dense_features[index], cat_features[index]\n",
    "    return dense_features, cat_features\n",
    "\n",
    "def get_preds(dense_features, cat_features, my_model, mode=0):\n",
    "    res = []\n",
    "    for m in my_model:\n",
    "        with torch.no_grad():\n",
    "            # Forward pass through the model to get predictions\n",
    "            y_pred = m.forward(dense_features, cat_features, mode).detach().numpy().flatten()\n",
    "            res.append(y_pred)\n",
    "    \n",
    "    # Average the predictions from multiple models\n",
    "    res = np.mean(res, axis=0)\n",
    "    return res\n",
    "\n",
    "def load_pkl(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        # Load and return the object stored in the pickle file\n",
    "        obj = pickle.load(f)\n",
    "        f.close()\n",
    "    return obj\n",
    "\n",
    "\n",
    "cat_cols = ['level', 'event_name', 'name', 'room_fqid', 'fqid', 'text_fqid', 'text']\n",
    "standard_dict = load_pkl('/kaggle/input/mastudentsmodels/standard_dict.pkl')\n",
    "feature_cols = standard_dict['feature_cols']\n",
    "cat_dict = load_pkl('/kaggle/input/mastudentsmodels/cat_dict.pkl')\n",
    "model_paths = ['../input/mastudentsmodels/fold{}.pth.tar'.format(i) for i in range(4)\n",
    "              ]\n",
    "my_models = [torch.load(path, map_location=torch.device('cpu')) for path in model_paths]\n",
    "\n",
    "def pred0(test, sample_submission, q_mode=0):\n",
    "    # Convert the test DataFrame to a pandas DataFrame\n",
    "    test = test.to_pandas()\n",
    "    \n",
    "    # Apply processing to the test DataFrame using the cat_dict\n",
    "    test = processing_df(test, cat_dict)\n",
    "    \n",
    "    # Extract features from the test DataFrame\n",
    "    test = get_features(test)\n",
    "    \n",
    "    # Normalize the dense features using the standard_dict\n",
    "    dense_features = normalize(test[feature_cols].values, feature_cols, standard_dict)\n",
    "    \n",
    "    # Get the categorical features from the test DataFrame\n",
    "    cat_features = test[cat_cols].values\n",
    "    \n",
    "    # If the number of dense features is greater than parameter.seq_length,\n",
    "    # perform random sampling to reduce the number of samples\n",
    "    if len(dense_features) > parameter.seq_length:\n",
    "        dense_features, cat_features = random_sample(dense_features, cat_features)\n",
    "        \n",
    "    # Convert the dense features to a torch tensor of type float32 and add a batch dimension\n",
    "    dense_features = torch.tensor(dense_features, dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "    # Convert the categorical features to a torch tensor of type long and add a batch dimension\n",
    "    cat_features = torch.tensor(cat_features, dtype=torch.long).unsqueeze(0)\n",
    "    \n",
    "    # Get predictions using the dense and categorical features, the my_models, and the specified q_mode\n",
    "    res = get_preds(dense_features, cat_features, my_models, q_mode)\n",
    "    \n",
    "    # Assign the predictions to the \"pred0\" column of the sample_submission DataFrame\n",
    "    sample_submission[\"pred0\"] = res\n",
    "    \n",
    "    return sample_submission\n",
    "\n",
    "\n",
    "level_limit = {'0-4':4, '5-12':12, '13-22':22}\n",
    "lgb_models1 = [lgb.Booster(model_file=f'/kaggle/input/mastudentsmodels/group0-4_lgb_fold{f}.txt') for f in range(4)]\n",
    "lgb_models2 = [lgb.Booster(model_file=f'/kaggle/input/mastudentsmodels/group5-12_lgb_fold{f}.txt') for f in range(4)]\n",
    "lgb_models3 = [lgb.Booster(model_file=f'/kaggle/input/mastudentsmodels/group13-22_lgb_fold{f}.txt') for f in range(4)]\n",
    "lgb_models = {'0-4': lgb_models1, '5-12': lgb_models2, '13-22': lgb_models3}\n",
    "\n",
    "def pred1(sample_submission, test, level_limit, texts_map):\n",
    "    # Rename the 'session_id' column in the test DataFrame to 'session'\n",
    "    test = test.rename({'session_id': 'session'})\n",
    "    \n",
    "    # Extract session ID, question number, and level group from the sample_submission DataFrame\n",
    "    sample_submission['session'] = sample_submission['session_id'].apply(lambda x: int(x.split('_')[0]))\n",
    "    sample_submission['question'] = sample_submission['session_id'].apply(lambda x: int(x.split('_')[-1].replace('q', '')))\n",
    "    sample_submission['level_group'] = sample_submission['question'].apply(lambda x: map_level_group(x))\n",
    "    \n",
    "    # Convert the sample_submission DataFrame to a PySpark DataFrame\n",
    "    sample_submission = pl.from_pandas(sample_submission)\n",
    "    \n",
    "    # Add features using the add_features function\n",
    "    features = add_features(sample_submission, test, level_limit, texts_map)\n",
    "    \n",
    "    # Fill missing columns in features with NaN values\n",
    "    for x in set(lgb_models[curr_lg][0].feature_name()) - set(features.columns):\n",
    "        features = features.with_columns(pl.lit(np.nan).alias(str(x)))\n",
    "    \n",
    "    # Convert the features DataFrame to a pandas DataFrame\n",
    "    features = features.to_pandas()\n",
    "    \n",
    "    preds = 0\n",
    "    for model in lgb_models[curr_lg]:\n",
    "        # Make predictions using each model in lgb_models\n",
    "        preds += model.predict(features[model.feature_name()]) / 4\n",
    "    \n",
    "    # Create a DataFrame with session ID, question number, and predictions\n",
    "    sub = features[['session', 'question']].copy()\n",
    "    sub['pred1'] = preds\n",
    "    sub['session_id'] = sub['session'].astype(str) + '_q' + sub['question'].astype(str)\n",
    "    sub = sub[['session_id', 'pred1']]\n",
    "    \n",
    "    return sub\n",
    "\n",
    "\n",
    "\n",
    "history_df = [None,None]  \n",
    "for (test, sample_submission) in iter_test:\n",
    "    test = test.sort_values(by=['index'], ascending=True)\n",
    "    sample_submission['question'] = sample_submission['session_id'].apply(lambda x: int(x.split('_')[-1].replace('q', '')))#.astype(int)\n",
    "    sample_submission = sample_submission.sort_values(by=['question'])\n",
    "    del sample_submission['question']\n",
    "    \n",
    "    curr_lg = test['level_group'].iloc[0]\n",
    "    test = pl.from_pandas(test)\n",
    "    \n",
    "    if curr_lg == '0-4':\n",
    "        q_mode = 0\n",
    "        history_df[0] = test.clone()\n",
    "    elif curr_lg == '5-12':\n",
    "        q_mode = 1\n",
    "        test = pl.concat([history_df[0], test])\n",
    "        history_df[1] = test.clone()\n",
    "    elif curr_lg == '13-22':\n",
    "        q_mode = 2\n",
    "        test = pl.concat([history_df[1], test])\n",
    "    else:\n",
    "        raise ValueError(curr_lg)\n",
    "    \n",
    "    sub0 = pred0(test.clone(), sample_submission.copy(), q_mode)\n",
    "    sub1 = pred1(sample_submission.copy(), test.clone(),  level_limit[curr_lg], texts_map[curr_lg])\n",
    "    sub_df = sub0.merge(sub1, how='inner',on='session_id')\n",
    "    sub_df['correct'] =  0.35 * sub_df['pred0'] + 0.65 * sub_df['pred1']\n",
    "    sub_df['correct'] = np.where(sub_df['correct']>0.625,1,0)\n",
    "    env.predict(sub_df[['session_id', 'correct']])\n",
    "print('ok')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 34.281891,
   "end_time": "2023-06-26T06:56:40.406243",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-26T06:56:06.124352",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
